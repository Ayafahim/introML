
## 1) Which histogram plots match which boxplots? #boxplot
![[Pasted image 20240508185706.png]]
![[Pasted image 20240508185734.png]]

From the histograms, we see that x3 (stearic) has a long right tail. For x8 (eicosenoic) more than a quarter of the observations are close to 0, which means that the first quartile must also be close to 0. Using this knowledge boxplot 2 is matched to x3 (stearic), and boxplot 4 is matched to x8 (eicosenoic).
Also you can see outliers in boxplot 2

## 2) Which one of the following matrices  is the correct empirical covariance matrix for these attributes? #Covariance

![[Pasted image 20240508193237.png]]

A. 
$$
\begin{bmatrix}
564.3 & -77.5 & 292.5 & -388.5 & 164.0 \\
-77.5 & 271.5 & -72.5 & 36.0 & -42.0 \\
292.5 & -72.5 & 392.4 & -324.8 & 248.1 \\
-388.5 & 36.0 & -324.8 & 369.9 & -241.4 \\
164.0 & -42.0 & 248.1 & -241.4 & 224.6 \\
\end{bmatrix}
$$

B.
$$
\begin{bmatrix}
-564.3 & -77.5 & 292.5 & -388.5 & 164.0 \\
-77.5 & -271.5 & -72.5 & 36.0 & -42.0 \\
292.5 & -72.5 & -392.4 & -324.8 & 248.1 \\
-388.5 & 36.0 & -324.8 & -369.9 & -241.4 \\
164.0 & -42.0 & 248.1 & -241.4 & -224.6 \\
\end{bmatrix}
$$

C.
$$
\begin{bmatrix}
224.6 & 248.1 & -42.0 & -241.4 & 164.0 \\
248.1 & 392.4 & -72.5 & -324.8 & 292.5 \\
-42.0 & -72.5 & 271.5 & 36.0 & -77.5 \\
-241.4 & -324.8 & 36.0 & 369.9 & -388.5 \\
164.0 & 292.5 & -77.5 & -388.5 & 564.3 \\
\end{bmatrix}
$$

D.
$$
\begin{bmatrix}
-224.6 & 248.1 & -42.0 & -241.4 & 164.0 \\
248.1 & -392.4 & -72.5 & -324.8 & 292.5 \\
-42.0 & -72.5 & -271.5 & 36.0 & -77.5 \\
-241.4 & -324.8 & 36.0 & -369.9 & -388.5 \\
164.0 & 292.5 & -77.5 & -388.5 & -564.3 \\
\end{bmatrix}
$$

We can rule out B & D since the diagonal is negative and that is not valid. Then you look eg. at $x_1,x_2$ in A its -77.5 which is negative and that does not match the plot since it is positively projected so it must be C.

## 3) Which one of the following statements is true? #PCA 

A Principal Component Analysis (PCA) is carried out on the Olive Oil dataset in Table 1 based on the attributes $x_1$, $x_2$, $x_3$, $x_4$ and $x_5$.

The data is standardized by (i) subtracting the mean and (ii) dividing each column by its standard deviation to obtain the standardized data matrix $\mathbf{\tilde{X}}$. A singular value decomposition is then carried out on the standardized data matrix to obtain the decomposition $\mathbf{U}\mathbf{S}\mathbf{V}^T = \mathbf{\tilde{X}}$

$$
\mathbf{V} = \begin{bmatrix}
0.48 & 0.09 & -0.57 & 0.52 & 0.42 \\
0.51 & 0.03 & -0.27 & -0.82 & 0.05 \\
-0.15 & 0.98 & 0.03 & -0.07 & 0.08 \\
-0.54 & -0.16 & -0.14 & 0.25 & 0.78 \\
0.45 & 0.01 & 0.77 & 0.05 & 0.46 \\
\end{bmatrix}
$$

$$
\mathbf{S} = \begin{bmatrix}
43.4 & 0.0 & 0.0 & 0.0 & 0.0 \\
0.0 & 23.39 & 0.0 & 0.0 & 0.0 \\
0.0 & 0.0 & 18.26 & 0.0 & 0.0 \\
0.0 & 0.0 & 0.0 & 9.34 & 0.0 \\
0.0 & 0.0 & 0.0 & 0.0 & 2.14 \\
\end{bmatrix}
$$

### Which one of the following statements is true?

A. The variance explained by the last four principal components is less than 0.3 of the total variance.

B. The variance explained by the first three principal components is greater than 0.9 of the total variance.

C. The variance explained by the first four principal components is less than 0.95 of the total variance.

D. The variance explained by the first principal component is greater than 0.715 of the total variance.

```python
import numpy as np  
  
# Singular values from the matrix S  
singular_values = np.array([43.4, 23.39, 18.26, 9.34, 2.14])  
  
# Square the singular values to get the variances  
variances = singular_values ** 2  
  
# Total variance is the sum of individual variances  
total_variance = np.sum(variances)  
  
# Proportion of variance explained by each principal component  
proportion_variance_explained = variances / total_variance  
  
# Compute cumulative variance explained by the first one, two, three, and four principal components  
cumulative_variance_one = np.sum(proportion_variance_explained[:1])  
cumulative_variance_three = np.sum(proportion_variance_explained[:3])  
cumulative_variance_four = np.sum(proportion_variance_explained[:4])  
  
# Cumulative variance explained by the last four principal components (excluding the first one)  
cumulative_variance_last_four = np.sum(proportion_variance_explained[1:])  
  
# Print the results  
print("Proportion of variance explained by each component:", proportion_variance_explained)  
print("Cumulative variance explained by one component:", cumulative_variance_one)  
print("Cumulative variance explained by three components:", cumulative_variance_three)  
print("Cumulative variance explained by four components:", cumulative_variance_four)  
print("Cumulative variance explained by the last four components:", cumulative_variance_last_four)  
  
# Evaluate the statements given in the problem  
A = cumulative_variance_last_four < 0.3  
B = cumulative_variance_three > 0.9  
C = cumulative_variance_four < 0.95  
D = cumulative_variance_one > 0.715  
  
# Print the truth value of each statement  
print("Statement A is", A)  
print("Statement B is", B)  
print("Statement C is", C)  
print("Statement D is", D)  
  
# Identify the correct statement  
statements = [A, B, C, D]  
statement_labels = ['A', 'B', 'C', 'D']  
for i, statement in enumerate(statements):  
    if statement:  
        print(f"The correct statement is: {statement_labels[i]}")
```


## 17) How many parameters has to be trained to fit the neural network? #ANN

An artificial neural network (ANN) trained on the Olive Oil dataset described in Table 1  will be used to predict the region of origin in Italy y as a multi-class classification problem based on all of the attributes x1, . . . , x8. The neural network has a single hidden layer containing nh = 50 units that uses a sigmoid non-linear activation function. The output layer uses a softmax activation function as described in the lecture notes, Section 15.3.2. How many parameters has to be trained to fit the neural network?
### Step-by-Step Calculation:

1. **Architecture Details**:
   - **Input Features (M)**: 8 (x1 through x8)
   - **Hidden Units (nh)**: 50
   - **Output Classes (C)**: 9 (one for each class)

2. **Parameters between Input Layer and Hidden Layer**:
   - Total parameters for the hidden layer = $(M + 1) \times nh = (8 + 1) \times 50 = 9 \times 50 = 450$

3. **Parameters between Hidden Layer and Output Layer**:
   - Total parameters for the output layer = $(nh + 1) \times C = (50 + 1) \times 9 = 51 \times 9 = 459$

4. **Total Parameters in the Network**:
   - Total parameters = $450 + 459 = 909$

### Conclusion:
The total number of parameters that need to be trained in this neural network is $909$.


