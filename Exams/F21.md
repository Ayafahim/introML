
## 1) Which histogram plots match which boxplots? #Boxplot
![[Pasted image 20240508185706.png]]
![[Pasted image 20240508185734.png]]

From the histograms, we see that x3 (stearic) has a long right tail. For x8 (eicosenoic) more than a quarter of the observations are close to 0, which means that the first quartile must also be close to 0. Using this knowledge boxplot 2 is matched to x3 (stearic), and boxplot 4 is matched to x8 (eicosenoic).
Also you can see outliers in boxplot 2

## 2) Which one of the following matrices  is the correct empirical covariance matrix for these attributes? #Covariance

![[Pasted image 20240508193237.png]]

A. 
$$
\begin{bmatrix}
564.3 & -77.5 & 292.5 & -388.5 & 164.0 \\
-77.5 & 271.5 & -72.5 & 36.0 & -42.0 \\
292.5 & -72.5 & 392.4 & -324.8 & 248.1 \\
-388.5 & 36.0 & -324.8 & 369.9 & -241.4 \\
164.0 & -42.0 & 248.1 & -241.4 & 224.6 \\
\end{bmatrix}
$$

B.
$$
\begin{bmatrix}
-564.3 & -77.5 & 292.5 & -388.5 & 164.0 \\
-77.5 & -271.5 & -72.5 & 36.0 & -42.0 \\
292.5 & -72.5 & -392.4 & -324.8 & 248.1 \\
-388.5 & 36.0 & -324.8 & -369.9 & -241.4 \\
164.0 & -42.0 & 248.1 & -241.4 & -224.6 \\
\end{bmatrix}
$$

C.
$$
\begin{bmatrix}
224.6 & 248.1 & -42.0 & -241.4 & 164.0 \\
248.1 & 392.4 & -72.5 & -324.8 & 292.5 \\
-42.0 & -72.5 & 271.5 & 36.0 & -77.5 \\
-241.4 & -324.8 & 36.0 & 369.9 & -388.5 \\
164.0 & 292.5 & -77.5 & -388.5 & 564.3 \\
\end{bmatrix}
$$

D.
$$
\begin{bmatrix}
-224.6 & 248.1 & -42.0 & -241.4 & 164.0 \\
248.1 & -392.4 & -72.5 & -324.8 & 292.5 \\
-42.0 & -72.5 & -271.5 & 36.0 & -77.5 \\
-241.4 & -324.8 & 36.0 & -369.9 & -388.5 \\
164.0 & 292.5 & -77.5 & -388.5 & -564.3 \\
\end{bmatrix}
$$

We can rule out B & D since the diagonal is negative and that is not valid. Then you look eg. at $x_1,x_2$ in A its -77.5 which is negative and that does not match the plot since it is positively projected so it must be C.

## 3) Which one of the following statements is true? #PCA 

A Principal Component Analysis (PCA) is carried out on the Olive Oil dataset in Table 1 based on the attributes $x_1$, $x_2$, $x_3$, $x_4$ and $x_5$.

The data is standardized by (i) subtracting the mean and (ii) dividing each column by its standard deviation to obtain the standardized data matrix $\mathbf{\tilde{X}}$. A singular value decomposition is then carried out on the standardized data matrix to obtain the decomposition $\mathbf{U}\mathbf{S}\mathbf{V}^T = \mathbf{\tilde{X}}$

$$
\mathbf{V} = \begin{bmatrix}
0.48 & 0.09 & -0.57 & 0.52 & 0.42 \\
0.51 & 0.03 & -0.27 & -0.82 & 0.05 \\
-0.15 & 0.98 & 0.03 & -0.07 & 0.08 \\
-0.54 & -0.16 & -0.14 & 0.25 & 0.78 \\
0.45 & 0.01 & 0.77 & 0.05 & 0.46 \\
\end{bmatrix}
$$

$$
\mathbf{S} = \begin{bmatrix}
43.4 & 0.0 & 0.0 & 0.0 & 0.0 \\
0.0 & 23.39 & 0.0 & 0.0 & 0.0 \\
0.0 & 0.0 & 18.26 & 0.0 & 0.0 \\
0.0 & 0.0 & 0.0 & 9.34 & 0.0 \\
0.0 & 0.0 & 0.0 & 0.0 & 2.14 \\
\end{bmatrix}
$$

### Which one of the following statements is true?

A. The variance explained by the last four principal components is less than 0.3 of the total variance.

B. The variance explained by the first three principal components is greater than 0.9 of the total variance.

C. The variance explained by the first four principal components is less than 0.95 of the total variance.

D. The variance explained by the first principal component is greater than 0.715 of the total variance.

```python
import numpy as np  
  
# Singular values from the matrix S  
singular_values = np.array([43.4, 23.39, 18.26, 9.34, 2.14])  
  
# Square the singular values to get the variances  
variances = singular_values ** 2  
  
# Total variance is the sum of individual variances  
total_variance = np.sum(variances)  
  
# Proportion of variance explained by each principal component  
proportion_variance_explained = variances / total_variance  
  
# Compute cumulative variance explained by the first one, two, three, and four principal components  
cumulative_variance_one = np.sum(proportion_variance_explained[:1])  
cumulative_variance_three = np.sum(proportion_variance_explained[:3])  
cumulative_variance_four = np.sum(proportion_variance_explained[:4])  
  
# Cumulative variance explained by the last four principal components (excluding the first one)  
cumulative_variance_last_four = np.sum(proportion_variance_explained[1:])  
  
# Print the results  
print("Proportion of variance explained by each component:", proportion_variance_explained)  
print("Cumulative variance explained by one component:", cumulative_variance_one)  
print("Cumulative variance explained by three components:", cumulative_variance_three)  
print("Cumulative variance explained by four components:", cumulative_variance_four)  
print("Cumulative variance explained by the last four components:", cumulative_variance_last_four)  
  
# Evaluate the statements given in the problem  
A = cumulative_variance_last_four < 0.3  
B = cumulative_variance_three > 0.9  
C = cumulative_variance_four < 0.95  
D = cumulative_variance_one > 0.715  
  
# Print the truth value of each statement  
print("Statement A is", A)  
print("Statement B is", B)  
print("Statement C is", C)  
print("Statement D is", D)  
  
# Identify the correct statement  
statements = [A, B, C, D]  
statement_labels = ['A', 'B', 'C', 'D']  
for i, statement in enumerate(statements):  
    if statement:  
        print(f"The correct statement is: {statement_labels[i]}")
```

## 4) Consider again the PCA analysis for the Olive Oil dataset, in particular the SVD decomposition of $\mathbf{\tilde{X}}$ in Equation (1). Which one of the following statements is true? #PCA 

A. An observation with a low value of $x_1$ (palmitic), a low value of $x_2$ (palmitoleic), a high value of $x_4$ (oleic), and a low value of $x_5$ (linoleic) will typically have a negative value of the projection onto principal component number 1. (Correct Answer)

B. An observation with a high value of $x_3$ (stearic) will typically have a negative value of the projection onto principal component number 2.

C. An observation with a low value of $x_1$ (palmitic), a high value of $x_2$ (palmitoleic), and a high value of $x_4$ (oleic) will typically have a positive value of the projection onto principal component number 4.

D. An observation with a low value of $x_1$ (palmitic), a low value of $x_2$ (palmitoleic), and a high value of $x_5$ (linoleic) will typically have a negative value of the projection onto principal component number 3.


### Steps to Solve the Problem:

1. **Examine the $V$ Matrix**: Look at the coefficients (entries) for each principal component (column in $V$). Positive coefficients indicate a positive correlation with the principal component and negative coefficients indicate a negative correlation.
2. **Consider the Sign and Magnitude of Coefficients**: The magnitude tells how much influence that variable has on the principal component. The sign (+/-) indicates the direction of the influence.
3. **Analyze Given Statements**:
   - For a statement that says "an observation with a low/high value of $x$ will have a positive/negative projection," you should:
     - **Low Value of $x$**: If $x$ has a negative coefficient in a principal component, a low value (which is numerically negative after centering and scaling) contributes positively to that component because of the multiplication of two negatives.
     - **High Value of $x$**: If $x$ has a positive coefficient in a principal component, a high value contributes positively to that component.

### Evaluating Statement A:
- **Statement A**: An observation with a low value of \( x1 \) (palmitic), a low value of \( x2 \) (palmitoleic), a high value of \( x4 \) (oleic), and a low value of \( x5 \) (linoleic) will typically have a negative value of the projection onto principal component number 1.
### Steps:
1. **Identify the First Principal Component**:
   - Look at the **first column** of the \( V \) matrix because each column corresponds to one principal component.

2. **Check the Coefficients**:
   - **For \( x1 \) and \( x2 \)**: Since you're given "low" values for these, and assuming these coefficients are positive in the first principal component, the negative signs of these "low" values (after mean subtraction and scaling) would contribute negatively to the projection.
   - **For \( x4 \)**: With a "high" value and assuming a positive coefficient, this would contribute positively.
   - **For \( x5 \)**: A "low" value here would contribute negatively if the coefficient is positive.

3. **Determine the Net Effect**:
   - If the first principal component's coefficients for \( x1 \) and \( x2 \) are positive, their "low" values contribute negatively.
   - If \( x4 \) has a positive coefficient, its "high" value contributes positively.
   - If \( x5 \) has a positive coefficient, its "low" value also contributes negatively.
   - Sum these influences (considering their signs). The predominant sign of the resulting sum will indicate whether the projection is generally positive or negative.

### Conclusion:
If the negative contributions from the "low" values of \( x1 \), \( x2 \), and \( x5 \) outweigh the positive contribution from the "high" value of \( x4 \), the overall projection onto the first principal component would be negative, making Statement A true if this is the case. However, the actual truth of the statement depends on the specific coefficients in the \( V \) matrix for the first principal component.

This approach lets you use the properties of the \( V \) matrix effectively to predict how changes in the original variables influence their representation in the transformed feature space created by PCA.

## 5) All the objects from four regions of origin are projected onto the first two principal components and visualised as a scatter plot in Figure 4. Which one of the following statements is true? #PCA

![[Pasted image 20240508203148.png]]

A Principal Component Analysis (PCA) is carried out on all the eight attributes of the Olive Oil dataset in Table 1. All the objects from four regions of origin are projected onto the first two principal components and visualised as a scatter plot in Figure 4. Which one of the following statements is true?

A. There exists a logistic regression classifier that takes the observations projected onto the first two principal components as input, which can binary classify the observations in the two regions South Apulia (y = 3) and Sicily (y = 4) with 0 error.

B. Any classifications tree using axis-aligned splits that takes the observation projected onto the first two principal components as input and binary classify the observations in the two regions South Apulia (y = 3) and Umbria (y = 9) has an error strictly greater than 0.

C. Any classification tree using axis-aligned splits that takes all eight attributes as input and binary classify the observations in the two regions South Apulia (y = 3) and Inner Sardinia (y = 5) has an error strictly greater than 0.

D. There exists a logistic regression classifier that takes all eight attributes as input, which can binary classify the observations in the two regions South Apulia (y = 3) and Umbria (y = 9) with 0 error.

### Explanation of Answers

- **Answer A** is incorrect, since the points of the two classes South Apulia (y = 3) and Sicily (y = 4) are not linearly separable in Figure 4.
- **Answer B** is incorrect, since a tree with two leafs (splitting e.g. around −1 in the projection onto the first principal component) will be able to perfectly classify the objects.
- **Answer C** is incorrect, since a classification tree is always able to obtain an error of 0 when there is no identical training object in the two classes (unless the tree complexity is limited).
- **Answer D** is correct, since the two classes South Apulia (y = 3) and Umbria (y = 9) are linearly separable in the PCA plot. Furthermore, if points are linearly separable in the projection onto the first two principal components, then they are also linearly separable in the original attribute space.

## 6) #TODO

## 7) A hierarchical clustering is applied to the 11 observations in Table 2 using maximum linkage. Which one of the dendrograms shown in Figure 5 corresponds to the distances given in Table 2? #Dendogram

Just use this script and then match it to the plots. OBS. it. might be mirrored.
```python
import numpy as np # type: ignore  
import matplotlib.pyplot as plt # type: ignore  
from scipy.cluster.hierarchy import dendrogram, linkage # type: ignore  
from scipy.spatial.distance import squareform # type: ignore  
  
correct_distance_matrix = np.array([  
    #1     #2    #3    #4     #5    #6    #7    #8    #9    #10    #11  
    [0.0, 53.8, 87.0, 67.4, 67.5, 71.2, 65.2, 117.9, 56.1, 90.3, 109.8],  
    [53.8, 0.0, 69.9, 75.5, 62.9, 58.0, 63.0, 135.0, 84.1, 107.9, 131.5],  
    [87.0, 69.9, 0.0, 49.7, 38.5, 19.3, 35.5, 91.8, 76.9, 78.7, 89.1],  
    [67.4, 75.5, 49.7, 0.0, 24.2, 47.2, 47.0, 62.3, 33.4, 37.2, 60.0],  
    [67.5, 62.9, 38.5, 24.2, 0.0, 37.7, 41.7, 79.5, 52.4, 60.2, 78.9],  
    [71.2, 58.0, 19.3, 47.2, 37.7, 0.0, 21.5, 95.6, 68.3, 78.4, 91.0],  
    [65.2, 63.0, 35.5, 47.0, 41.7, 21.5, 0.0, 96.0, 64.3, 75.5, 89.4],  
    [117.9, 135.0, 91.8, 62.3, 79.5, 95.6, 96.0, 0.0, 66.9, 44.3, 24.2],  
    [56.1, 84.1, 76.9, 33.4, 52.4, 68.3, 64.3, 66.9, 0.0, 39.2, 60.7],  
    [90.3, 107.9, 78.7, 37.2, 60.2, 78.4, 75.5, 44.3, 39.2, 0.0, 39.4],  
    [109.8, 131.5, 89.1, 60.0, 78.9, 91.0, 89.4, 24.2, 60.7, 39.4, 0.0]  
])  
correct_distance_matrix = (correct_distance_matrix + correct_distance_matrix.T) / 2  
# Set diagonal to zero  
np.fill_diagonal(correct_distance_matrix, 0)  
  
# Convert the distance matrix to a condensed form  
condensed_matrix = squareform(correct_distance_matrix)  
  
# Perform hierarchical clustering using complete linkage  
  
# TODO: If the question is minimum/single linkage use 'single' otherwise if maximum/complete linkage use 'complete' in the second parameter  
linked = linkage(condensed_matrix, 'complete')  
  
# Plot the dendrogram  
plt.figure(figsize=(10, 7))  
dendrogram(linked,  
           orientation='top',  
           labels=range(1, 12), # TODO: Remember to change the range corresponding to the size of matrix/table (if it's 9x9 then range is 1,10 if 10x10 range it 1,11)  
           distance_sort='descending',  
           show_leaf_counts=True)  
plt.title('Dendrogram for Hierarchical Clustering')  
plt.xlabel('Index of Point')  
plt.ylabel('Distance')  
plt.show()
```

## 8) To examine if observation $o_5$ may be an outlier, we will calculate the K-nearest neighbor density using only the observations and distances in Table 2. For an observation $o_i$, recall the density is computed using the set of K nearest neighbors of observation $o_i$, excluding the $i^{th}$ observation itself, $N_{X_i} (o_i, K)$, and is denoted by `density_{X_i} (o_i, K)`. What is the density for observation $o_5$ for $K = 3$ nearest neighbors? #EuclianDistances 

```python

import numpy as np # type: ignore  
import matplotlib.pyplot as plt # type: ignore  
from scipy.cluster.hierarchy import dendrogram, linkage # type: ignore  
from scipy.spatial.distance import squareform # type: ignore  
  
correct_distance_matrix = np.array([  
    #1     #2    #3    #4     #5    #6    #7    #8    #9    #10    #11  
    [0.0, 53.8, 87.0, 67.4, 67.5, 71.2, 65.2, 117.9, 56.1, 90.3, 109.8],  
    [53.8, 0.0, 69.9, 75.5, 62.9, 58.0, 63.0, 135.0, 84.1, 107.9, 131.5],  
    [87.0, 69.9, 0.0, 49.7, 38.5, 19.3, 35.5, 91.8, 76.9, 78.7, 89.1],  
    [67.4, 75.5, 49.7, 0.0, 24.2, 47.2, 47.0, 62.3, 33.4, 37.2, 60.0],  
    [67.5, 62.9, 38.5, 24.2, 0.0, 37.7, 41.7, 79.5, 52.4, 60.2, 78.9],  
    [71.2, 58.0, 19.3, 47.2, 37.7, 0.0, 21.5, 95.6, 68.3, 78.4, 91.0],  
    [65.2, 63.0, 35.5, 47.0, 41.7, 21.5, 0.0, 96.0, 64.3, 75.5, 89.4],  
    [117.9, 135.0, 91.8, 62.3, 79.5, 95.6, 96.0, 0.0, 66.9, 44.3, 24.2],  
    [56.1, 84.1, 76.9, 33.4, 52.4, 68.3, 64.3, 66.9, 0.0, 39.2, 60.7],  
    [90.3, 107.9, 78.7, 37.2, 60.2, 78.4, 75.5, 44.3, 39.2, 0.0, 39.4],  
    [109.8, 131.5, 89.1, 60.0, 78.9, 91.0, 89.4, 24.2, 60.7, 39.4, 0.0]  
])  
correct_distance_matrix = (correct_distance_matrix + correct_distance_matrix.T) / 2  
# Set diagonal to zero  
np.fill_diagonal(correct_distance_matrix, 0)  
  
# Convert the distance matrix to a condensed form  
condensed_matrix = squareform(correct_distance_matrix)  
  
# Extract distances for observation o5 (index 4 because Python uses 0-based indexing)  
distances_o5 = correct_distance_matrix[4]  
  
# Find the distances to the nearest neighbors, excluding the zero distance to itself  
# We will sort the distances and select the smallest three that are not zero  
nearest_distances = np.sort(distances_o5[distances_o5 > 0])[:3]  
  
# Calculate the average distance to the 3 nearest neighbors  
average_distance = np.mean(nearest_distances)  
  
# Calculate the density  
density_o5 = 1 / average_distance  
  
# Print the density  
print(f"The density for observation o5 with K=3 nearest neighbors is: {density_o5:.3f}")

```

## 9) What is the estimated density at $O11$ using these assumptions? #EuclianDistances 

Consider again the distances calculated from the Olive Oil dataset in Table 1 with $M = 8$ features. We wish to apply kernel density estimation for observations in the dataset. Apply kernel density estimation for the observation $o_{11}$, where only the closest two observations are used to estimate the kernel density and excluding $o_{11}$. Set the kernel width $\lambda = 20$. What is the estimated density at $o_{11}$ using these assumptions?

A. $p_\lambda(o_{11}) \approx \frac{1}{2} \cdot \frac{1}{\sqrt{(2\pi \cdot 20^2)^8}} \cdot 0.6246$ (Answer)

B. $p_\lambda(o_{11}) \approx \frac{1}{2} \cdot \frac{1}{\sqrt{(2\pi \cdot 20^2)^8}} \cdot 1.922$

C. $p_\lambda(o_{11}) \approx \frac{1}{\sqrt{(2\pi \cdot 20^2)^8}} \cdot 0.6246$

D. $p_\lambda(o_{11}) \approx \frac{1}{\sqrt{(2\pi \cdot 20^2)^8}} \cdot 1.922$


```python
import numpy as np # type: ignore

correct_distance_matrix = np.array([
    [0.0, 53.8, 87.0, 67.4, 67.5, 71.2, 65.2, 117.9, 56.1, 90.3, 109.8],
    [53.8, 0.0, 69.9, 75.5, 62.9, 58.0, 63.0, 135.0, 84.1, 107.9, 131.5],
    [87.0, 69.9, 0.0, 49.7, 38.5, 19.3, 35.5, 91.8, 76.9, 78.7, 89.1],
    [67.4, 75.5, 49.7, 0.0, 24.2, 47.2, 47.0, 62.3, 33.4, 37.2, 60.0],
    [67.5, 62.9, 38.5, 24.2, 0.0, 37.7, 41.7, 79.5, 52.4, 60.2, 78.9],
    [71.2, 58.0, 19.3, 47.2, 37.7, 0.0, 21.5, 95.6, 68.3, 78.4, 91.0],
    [65.2, 63.0, 35.5, 47.0, 41.7, 21.5, 0.0, 96.0, 64.3, 75.5, 89.4],
    [117.9, 135.0, 91.8, 62.3, 79.5, 95.6, 96.0, 0.0, 66.9, 44.3, 24.2],
    [56.1, 84.1, 76.9, 33.4, 52.4, 68.3, 64.3, 66.9, 0.0, 39.2, 60.7],
    [90.3, 107.9, 78.7, 37.2, 60.2, 78.4, 75.5, 44.3, 39.2, 0.0, 39.4],
    [109.8, 131.5, 89.1, 60.0, 78.9, 91.0, 89.4, 24.2, 60.7, 39.4, 0.0]
])

# Distances for observation o11 from the distance matrix
distances_o11 = correct_distance_matrix[10]  # Index 10 corresponds to o11

# Identify the two smallest distances (excluding the distance to itself, which is 0)
sorted_indices = np.argsort(distances_o11)
closest_two_indices = sorted_indices[1:3]  # Skip the first one as it is the distance to itself

# Distances of the two closest observations to o11
closest_distances = distances_o11[closest_two_indices]

# Number of features M
M = 8

# Bandwidth lambda
lambda_ = 20

# Gaussian kernel density estimation
kde_estimate = (1 / (2 * np.sqrt((2 * np.pi * lambda_**2)**M))) * np.sum(np.exp(-closest_distances**2 / (2 * lambda_**2)))

print(kde_estimate, closest_distances)


# TODO CHECK THESE OPTIONS FROM THE ANSWERS
lambda_ = 20
M = 8
two_pi_lambda_squared = (2 * np.pi * lambda_**2)**M

# Calculate the denominator for the density estimations
denominator = np.sqrt(two_pi_lambda_squared)

# Compute the estimates for each option
option_A = 0.5 / denominator * 0.6246
option_B = 0.5 / denominator * 1.922
option_C = 1 / denominator * 0.6246
option_D = 1 / denominator * 1.922

print("A: ", option_A)
print("B: ", option_B)
print("C: ", option_C)
print("D: ", option_D)
```

----------------------------
```
#7.826826302434703e-15 [24.2 39.4]
#A:  7.827310200532026e-15
#B:  2.4085959342655385e-14
#C:  1.5654620401064053e-14
#D:  4.817191868531077e-14
```



## 10) Now, we consider the binarized version  of the Olive Oil dataset in Table 3. According to this  dataset, what is the probability that a sample comes  from the region Calabria given that we in that sample observe that the palmitic content is below the median and that the arachidic content is above the median? #BinaryVersion

A. $p(C_2 \mid f_1 = 0, f_6 = 1) = \frac{5}{11}$

B. $p(C_2 \mid f_1 = 0, f_6 = 1) = \frac{4}{7}$

C. $p(C_2 \mid f_1 = 0, f_6 = 1) = \frac{5}{7}$ (Correct)

D. $p(C_2 \mid f_1 = 0, f_6 = 1) = 1$


```python
import numpy as np # type: ignore  
  
# Define the binarized dataset with corrected understanding or labels if necessary  
data = np.array([  
    [0, 0, 0, 1, 0, 0, 0, 1],  # o1 -> C1  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o2 -> C1  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o3 -> C2  
    [0, 1, 0, 0, 0, 1, 0, 1],  # o4 -> C2  
    [0, 0, 0, 0, 0, 1, 0, 1],  # o5 -> C2  
    [0, 0, 1, 0, 1, 1, 0, 1],  # o6 -> C2  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o7 -> C2  
    [1, 1, 0, 0, 0, 0, 1, 1],  # o8 -> C3  
    [0, 1, 0, 0, 0, 0, 0, 1],  # o9 -> C3  
    [0, 1, 0, 0, 0, 1, 0, 1],  # o10 -> C3  
    [1, 1, 0, 0, 0, 0, 0, 0],  # o11 -> C3  
])  
  
# Define the class labels for the data  
classes = np.array([1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3])  # Classes corresponding to each observation  
  
# Filter the data where f1 = 0 and f6 = 1  
mask_f1_0_f6_1 = (data[:, 0] == 0) & (data[:, 5] == 1)  
  
# Filtered class labels for the condition f1 = 0 and f6 = 1  
filtered_classes = classes[mask_f1_0_f6_1]  
  
# Count the occurrences of class C2 in the filtered dataset  
count_C2 = np.sum(filtered_classes == 2)  
  
# Total number of samples meeting the condition f1 = 0 and f6 = 1  
total_filtered = len(filtered_classes)  
  
# Calculate the conditional probability p(C2 | f1 = 0, f6 = 1)  
conditional_probability = count_C2 / total_filtered if total_filtered > 0 else 0  
print(conditional_probability, count_C2, total_filtered)
```
----
```
0.7142857142857143 5 7
```

## 11) Consider the observations in Table 3. We consider these as 8-dimensional binary vectors and wish to compute the pairwise similarity. Which one of the following statements is true? #jaccard #BinaryVersion 

```python
import numpy as np  
  
def cosine_similarity(vec1, vec2):  
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))  
  
  
def simple_matching_coefficient(vec1, vec2):  
    return sum(1 for i, j in zip(vec1, vec2) if i == j) / len(vec1)  
  
  
def jaccard_similarity(vec1, vec2):  
    intersection = sum(1 for i, j in zip(vec1, vec2) if i == j and i == 1)  
    union = sum(1 for i, j in zip(vec1, vec2) if i == 1 or j == 1)  
    return intersection / union  
  
  
# Observations from Table 4 as 5-dimensional binary vectors  
o1 = np.array([0, 0, 0, 1, 0, 0, 0, 1])  
o2 = np.array([0, 0, 1, 0, 0, 1, 0, 1])  
o3 = np.array([0, 0, 1, 0, 0, 1, 0, 1])  
o4 = np.array([0, 1, 0, 0, 0, 1, 0, 1])  
  
# Calculate similarities and coefficients  
print(simple_matching_coefficient(o2, o4))  
print(cosine_similarity(o1, o2))  
print(simple_matching_coefficient(o3, o4))  
print(jaccard_similarity(o2, o4))
```

## 12) Consider again the binary data presented in Table 3 with three classes. We will use Hunt’s algorithm to construct a classification tree using the Gini impurity measure. Suppose that the data in Table 3 is at the root node, and a binary split is made based on two different values of $f_2$. What is the impurity gain of this split? #HuntsAlgo 

```python
from fractions import Fraction  
import numpy as np  # type: ignore  
  
# Data from the table, with class labels assigned based on the provided information  
data = np.array([  
    [0, 0, 0, 1, 0, 0, 0, 1],  # o1 -> C1  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o2 -> C1  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o3 -> C2  
    [0, 1, 0, 0, 0, 1, 0, 1],  # o4 -> C2  
    [0, 0, 0, 0, 0, 1, 0, 1],  # o5 -> C2  
    [0, 0, 1, 0, 1, 1, 0, 1],  # o6 -> C2  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o7 -> C2  
    [1, 1, 0, 0, 0, 0, 1, 1],  # o8 -> C3  
    [0, 1, 0, 0, 0, 0, 0, 1],  # o9 -> C3  
    [0, 1, 0, 0, 0, 1, 0, 1],  # o10 -> C3  
    [1, 1, 0, 0, 0, 0, 0, 0],  # o11 -> C3  
])  
  
# Class labels as described  
class_labels = np.array([1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3])  
  
  
# Gini impurity calculation  
def gini_impurity(classes):  
    _, counts = np.unique(classes, return_counts=True)  
    probabilities = counts / counts.sum()  
    return 1 - np.sum(probabilities ** 2)  
  
  
# Compute Gini impurity for the root node  
root_impurity = gini_impurity(class_labels)  
  
# Compute Gini impurities for the split on f2  
mask_f2_0 = data[:, 1] == 0  
mask_f2_1 = data[:, 1] == 1  
  
gini_f2_0 = gini_impurity(class_labels[mask_f2_0])  
gini_f2_1 = gini_impurity(class_labels[mask_f2_1])  
  
# Calculate the proportions for weighted average  
prop_f2_0 = mask_f2_0.sum() / len(class_labels)  
prop_f2_1 = mask_f2_1.sum() / len(class_labels)  
  
# Weighted Gini impurity after the split  
weighted_gini = prop_f2_0 * gini_f2_0 + prop_f2_1 * gini_f2_1  
  
# Impurity gain from the split  
gini_gain = root_impurity - weighted_gini  
  
print(root_impurity, gini_f2_0, gini_f2_1, weighted_gini, gini_gain)  
fraction = Fraction(gini_gain).limit_denominator()  
  
# Print the probability and the fraction  
print("Probability:", gini_gain)  
print("Fraction:", fraction)
```

## 13) We consider the binary matrix from Table 3 as a market basket problem consisting of $N = 11$ transactions $o_1, ..., o_{11}$ and $M = 8$ items $f_1, ..., f_8$. What is the **confidence** of the rule $\{f_6, f_8\} \rightarrow \{f_3, f_5\}$? #BinaryVersion 

A. The confidence is $\frac{1}{11}$

B. The confidence is $\frac{1}{7}$

C. The confidence is $\frac{4}{11}$

D. The confidence is 1

```python
import numpy as np  
from fractions import Fraction  
  
# Binary matrix with each row representing a transaction  
binary_matrix = np.array([  
    [0, 0, 0, 1, 0, 0, 0, 1],  # o1  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o2  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o3  
    [0, 1, 0, 0, 0, 1, 0, 1],  # o4  
    [0, 0, 0, 0, 0, 1, 0, 1],  # o5  
    [0, 0, 1, 0, 1, 1, 0, 1],  # o6  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o7  
    [1, 1, 0, 0, 0, 0, 1, 1],  # o8  
    [0, 1, 0, 0, 0, 0, 0, 1],  # o9  
    [0, 1, 0, 0, 0, 1, 0, 1],  # o10  
    [1, 1, 0, 0, 0, 0, 0, 0]   # o11  
])  
  
# Check which rows have both f6=1 and f8=1  
antecedent = (binary_matrix[:, 5] == 1) & (binary_matrix[:, 7] == 1)  
  
# Check which of those rows also have f3=1 and f5=1  
consequent = (binary_matrix[:, 2] == 1) & (binary_matrix[:, 4] == 1)  
combined = antecedent & consequent  
  
# Calculate support values  
support_antecedent = np.sum(antecedent)  
support_combined = np.sum(combined)  
  
# Calculate confidence  
confidence = support_combined / support_antecedent if support_antecedent != 0 else 0  
  
# Output as fraction  
confidence_fraction = Fraction(int(support_combined), int(support_antecedent)).limit_denominator()  
  
print(f"Support of Antecedent: {support_antecedent}")  
print(f"Support of Combined: {support_combined}")  
print(f"Confidence (decimal): {confidence:.3f}")  
print(f"Confidence (fraction): {confidence_fraction}")
```

## 14) #TODO 

## 15) What is then the probability that the oil comes from the region North Apulia (C1) according to the naïve-Bayes classifier? #naiveBayes


![[Pasted image 20240508212230.png]]

Consider the small subset of the Olive Oil dataset shown in Table 4. Suppose we train a naïve-Bayes classifier on this subset to predict the class label $y$ from only the attributes $x_1$ and $x_2$. In this naïve-Bayes classifier, we assume that the conditional density of each attribute is a 1D Gaussian, $$ p(x_i \mid C_j) = \mathcal{N}(x_i \mid \mu_{j,i}, \sigma^2), $$ where $\mu_{j,i}$ is the mean of the $i^{th}$ feature for class $j$. We will assume that $\sigma^2 = 400$ for all attributes and all classes. For a test Olive Oil sample, we observe that $x_1 = 32.0$, $x_2 = 14.0$. Furthermore, you can assume that the value of denominator in the calculation of the class-probabilities using the naïve-Bayes classifier is $$ p_{NB}(x_1 = 15.0, x_2 = 14.0) = 0.00010141 $$ What is then the probability that the oil comes from the region North Apulia (C1) according to the naïve-Bayes classifier?
A. $p_{NB}(C_1 \mid x_1 = 32.0, x_2 = 14.0) \approx 59\%$ 
B. $p_{NB}(C_1 \mid x_1 = 32.0, x_2 = 14.0) \approx 71\%$ 
C. $p_{NB}(C_1 \mid x_1 = 32.0, x_2 = 14.0) \approx 84\%$ 
D. $p_{NB}(C_1 \mid x_1 = 32.0, x_2 = 14.0) \approx 96\%$


```python
import numpy as np # type: ignore  
  
  
data = np.array([  
    [38.0, 15.1, 27.4, 77.9, 18.1, 33.3, 48.5, 50.0],  
    [26.8, 12.8, 52.0, 77.0, 22.5, 68.1, 66.0, 75.0],  
    [64.5, 39.6, 74.4, 37.1, 45.7, 66.7, 66.0, 64.3],  
    [63.2, 45.7, 29.1, 41.4, 49.1, 56.9, 59.2, 50.0],  
    [66.3, 34.3, 37.7, 43.1, 40.9, 63.9, 70.9, 60.7],  
    [56.7, 34.7, 72.2, 47.3, 38.4, 61.1, 62.1, 55.4],  
    [63.4, 30.6, 66.4, 49.8, 30.2, 62.5, 50.5, 42.9],  
    [87.1, 85.3, 19.3, 19.2, 68.6, 34.7, 64.1, 33.9],  
    [51.3, 46.8, 14.8, 53.4, 49.3, 37.5, 52.4, 35.7],  
    [67.5, 62.3, 13.0, 33.2, 66.7, 51.4, 41.7, 39.3],  
    [86.0, 71.3, 25.1, 20.5, 71.9, 25.0, 48.5, 32.1],  
])  
  
  
# Means for x1 and x2 in class C1  
mu_x1_c1 = (38.0 + 26.8) / 2  
mu_x2_c1 = (15.1 + 12.8) / 2  
  
# Class probability for C1  
p_C1 = 2 / 11  
  
# Variance (sigma squared) for Gaussian distributions  
sigma_squared = 400  
  
# Gaussian density function  
def gaussian_density(x, mu, sigma_squared):  
    return (1 / np.sqrt(2 * np.pi * sigma_squared)) * np.exp(-((x - mu)**2) / (2 * sigma_squared))  
  
# Calculate densities for x1 = 32.0 and x2 = 14.0 for class C1  
p_x1_given_c1 = gaussian_density(32.0, mu_x1_c1, sigma_squared)  
p_x2_given_c1 = gaussian_density(14.0, mu_x2_c1, sigma_squared)  
  
# Given denominator value for the Naive Bayes classification  
p_NB_denominator = 0.00010141  
  
# Calculate numerator of the Bayes rule  
numerator = p_x1_given_c1 * p_x2_given_c1 * p_C1  
  
# Calculate posterior probability for C1  
posterior_c1 = numerator / p_NB_denominator  
  
print(posterior_c1)
```

## 16) Consider a two-dimensional data set comprised of N = 8 observations shown in Figure 6.  The dataset consists of three classes indicated by the blue squares (class 1), red triangles (class 2) and yellow circles (class 3). In the figure, the decision boundaries for four K-nearest neighbor classifiers (KNN) are shown. Which one of the plots correspond to the K = 3 nearest-neighbour classifier assuming ties are broken by assigning to the nearest neighbour’s class? #KNN

![[Pasted image 20240508212515.png]]


The point (−1, 0) must be assigned to class 2, because there is a tie between all three classes  
and the nearest neighbour belongs to class 2. This rules out options A and B. Points close to the rightmost blue square must be assigned to class 2, since the two nearest neighbours belong to class 2. This rules out option C. Therefore D is the correct answer.
## 17) How many parameters has to be trained to fit the neural network? #ANN

An artificial neural network (ANN) trained on the Olive Oil dataset described in Table 1  will be used to predict the region of origin in Italy y as a multi-class classification problem based on all of the attributes x1, . . . , x8. The neural network has a single hidden layer containing nh = 50 units that uses a sigmoid non-linear activation function. The output layer uses a softmax activation function as described in the lecture notes, Section 15.3.2. How many parameters has to be trained to fit the neural network?
### Step-by-Step Calculation:

1. **Architecture Details**:
   - **Input Features (M)**: 8 (x1 through x8)
   - **Hidden Units (nh)**: 50
   - **Output Classes (C)**: 9 (one for each class)

2. **Parameters between Input Layer and Hidden Layer**:
   - Total parameters for the hidden layer = $(M + 1) \times nh = (8 + 1) \times 50 = 9 \times 50 = 450$

3. **Parameters between Hidden Layer and Output Layer**:
   - Total parameters for the output layer = $(nh + 1) \times C = (50 + 1) \times 9 = 51 \times 9 = 459$

4. **Total Parameters in the Network**:
   - Total parameters = $450 + 459 = 909$

### Conclusion:
The total number of parameters that need to be trained in this neural network is $909$.

## 18) Which one of the following values of $w^*$ minimizes the mean squared error? #ANN

Consider a two-layer neural network $f : \mathbb{R}^2 \rightarrow \mathbb{R}$ for regression with one hidden unit and that can be written in the form

$$
f_{w(1),w(2)}(x) = z^{(1)} w^{(2)},
$$

where $\tilde{x} = [1 \ x_1 \ x_2]^T$, $z^{(1)} \in \mathbb{R}$, $z^{(1)} = [1 \ z^{(1)}_1]$, and $h^{(1)}(x) = \max(0, x)$ is the activation function for the hidden layer (rectified linear unit). Assume that the weights of the first layer are fixed and given by

$$
w^{(1)T} = [-2 \ 4 \ 2]
$$

Given $N$ observations $x_1, x_2, \ldots, x_N$ and corresponding targets $y_1, y_2, \ldots, y_N$, our learning objective is to find the value of the weight for the second layer $w^{(2)}$ that minimizes the mean squared error,

$$
w^* = \arg\min_{w^{(2)}} \frac{1}{N} \sum_{i=1}^{N} \left[ f_{w(1),w(2)}(x_i) - y_i \right]^2,
$$

where $w^* = [w^*_1 \ w^*_2]^T \in \mathbb{R}^2$.

Consider the following dataset with $N = 4$ observations in $X$ and the corresponding 4 targets in $y$:

$$
X = \begin{bmatrix}
0 & 0 \\
1 & 0 \\
1 & 1 \\
1 & 2
\end{bmatrix},
y = \begin{bmatrix}
1 \\
3 \\
5 \\
7
\end{bmatrix}
$$

Which one of the following values of $w^*$ minimizes the mean squared error?

A. $w^* = [1 \ 1]^T$

B. $w^* = [1 \ 2]^T$

C. $w^* = [1 \ 3]^T$

D. $w^* = [1 \ 4]^T$

### Step-by-Step Explanation:

1. **Input Preparation**:
   - $X$: The input features.
   - $y$: The target outputs.
   - $X_{\text{augmented}}$: Adds a bias term to the input $X$. Since the weights $w^{(1)}$ include a bias term, augmenting $X$ with a column of ones allows for a dot product that incorporates this bias.

2. **First Layer Transformation**:
   - Compute $\tilde{x} \cdot w^{(1)T}$ which gives the input to the ReLU activation function for each example.
   - Apply ReLU, which outputs zero if the input is less than zero and the input itself if it's non-negative.

3. **Testing Different Weights for the Second Layer**:
   - You've considered different values for $w^{(2)}$ (which are just scalars here given that there's only one unit in the hidden layer outputting to the final node).
   - The network's output is $f(x) = z^{(1)} \times w^{(2)}$, where $z^{(1)}$ is the output from the first layer (after applying ReLU).
   
4. **MSE Calculation**:
   - For each candidate $w^{(2)}$, calculate the network's predicted outputs.
   - Compute the MSE between these predictions and the actual targets $y$.

5. **Select the Best $w^{(2)}$**:
   - The MSEs you calculated are: `{A: 1.0, B: 9.0, C: 45.0, D: 109.0}`.
   - Lower MSE indicates a better fit. Therefore, the option with the lowest MSE (1.0 for option A) is the best among the given options.

### Conclusion:

Based on the MSE values computed, the best weight for the second layer, minimizing the MSE, is **Option A** with $w^{(2)} = [1]$, corresponding to an MSE of 1.0. This outcome means that using $w^{(2)} = 1$ gives predictions closest to the target values $y$ among the options tested, thus making it the most effective choice for this particular setup.

The calculation and process followed in your script are correctly aligned with solving this type of problem in a neural network with a single hidden unit using a scalar for the second layer's weights. The script effectively demonstrates how to evaluate different model configurations systematically to identify the one that best fits the data according to the MSE metric.



```python
import numpy as np # type: ignore  
  
# Input data and corresponding targets  
X = np.array([[0, 0], [1, 0], [1, 1], [1, 2]])  
y = np.array([1, 3, 5, 7])  
  
# Fixed weights for the first layer and activation function is ReLU  
w1 = np.array([-2, 4, 2])  # Last element is the bias  
X_augmented = np.hstack((np.ones((X.shape[0], 1)), X))  # Augment X with a column of ones for the bias term  
  
# Compute the activations from the first layer  
z1_input = X_augmented.dot(w1)  
z1 = np.maximum(0, z1_input)  # Apply ReLU activation function  
  
# Define candidate weights for the second layer, assuming the question intends only the scalar to vary  
w2_options = {  
    'A': np.array([1]),  
    'B': np.array([2]),  
    'C': np.array([3]),  
    'D': np.array([4])  
}  
  
# Calculate MSE for each option  
mse_results = {}  
for key, w2 in w2_options.items():  
    f_x = z1 * w2  # Output of the network  
    mse = np.mean((f_x - y) ** 2)  
    mse_results[key] = mse  
  
# REMEMBER TO CHECK IF WE ARE LOOKING AT MINIMIZE OR MAXIMIZE  
print(mse_results)
```

## 19) What is the value of the weight of a correctly classified observation $i$ after the first round of boosting? #AdaBoost 

Consider again the Olive Oil dataset of Table 1. Suppose we wish to predict the class label $y$ using a decision tree model, and to improve performance we wish to apply AdaBoost. We apply AdaBoost to the full Olive Oil dataset. Recall the first steps of AdaBoost consists of: (i) Initialize weights, (ii) select a subset for training using sampling with replacement, and (iii) fit a model to the training set. Suppose the first fitted model has an accuracy of $\frac{3}{4}$ on the full dataset, what is the value of the weight of a correctly classified observation $i$ after the first round of boosting?

**A.** $w_i(2) = \frac{2}{3} \cdot \frac{1}{572}$ (correct)

**B.** $w_i(2) = \frac{3}{4} \cdot \frac{1}{572}$

**C.** $w_i(2) = \frac{4}{5} \cdot \frac{1}{572}$

**D.** $w_i(2) = \frac{5}{6} \cdot \frac{1}{572}$

```python
import numpy as np  
  
# Number of observations - not given explicitly, assume N if needed, or use the formula directly  
# For example purposes, let's assume N = 572 as the question implies weights are normalized by 1/572  
  
N = 572  
initial_weight = 1 / N  
  
# Error of the classifier (25% misclassified)  
epsilon = 0.25  
  
# Calculate alpha (performance of the classifier)  
alpha = 0.5 * np.log((1 - epsilon) / epsilon)  
  
# Updated weights calculation  
# Correctly classified observations  
weight_update_correct = initial_weight * np.exp(-alpha)  
  
# Normalize weights  
# Since we know the ratio of correctly vs incorrectly classified,  
# We could directly calculate normalization, but here, use direct approach:  
# Assume all start with the same weight and 75% are correct, 25% incorrect  
weights = np.full(N, initial_weight)  
weights[:int(0.75 * N)] *= np.exp(-alpha)  
weights[int(0.75 * N):] *= np.exp(alpha)  
  
# Normalize weights  
weights /= np.sum(weights)  
  
# Find the normalized weight of a correctly classified observation  
correct_weight_normalized = weights[0]  
  
print("Updated normalized weight of a correctly classified observation:", correct_weight_normalized)
```

## 20) Suppose the first selected centroid is $\mu_1 = 1.7$, what are the locations of the next two centroids? #misc

Consider a small dataset comprised of $N = 4$ observations:

$$
x = [0.4 \quad 1.7 \quad 3.7 \quad 4.6]^T.
$$

We wish to apply the k-means algorithm to the dataset using $K = 3$ and the farthest-first initialization method described in Section 18.2.2. Suppose the first selected centroid is $\mu_1 = 1.7$, what are the locations of the next two centroids?


The **farthest-first initialization** method, also known as the k-means++ initialization technique, is used in k-means clustering to help improve the likelihood of finding a more optimal clustering by spreading out the initial centroids. This initialization method can significantly influence the clustering process by avoiding poor convergence and local minima that the standard k-means algorithm can suffer from when centroids are initialized randomly.

### Process of Farthest-First Initialization:

1. **Select the first centroid**:
   - The first centroid $( \mu_1 )$ is chosen at random or based on a specific criterion. In your case, it's given as $( \mu_1 = 1.7)$.

2. **Select subsequent centroids**:
   - Each subsequent centroid is chosen to be the point in the dataset that is farthest from the nearest already chosen centroid. This is calculated based on the maximum distance from the current set of centroids.

### Given Problem and Steps to Solution:

1. **First Centroid**: $( \mu_1 = 1.7)$

2. **Find the Second Centroid** $(\mu_2)$:
   - Calculate the distance of each data point from $( \mu_1)$ and pick the farthest.
   - Distances:
     - Distance from 0.4 to 1.7: $( |0.4 - 1.7| = 1.3)$
     - Distance from 3.7 to 1.7: $( |3.7 - 1.7| = 2.0)$
     - Distance from 4.6 to 1.7: $( |4.6 - 1.7| = 2.9)$
   - The farthest point from $( \mu_1 = 1.7 )$ is 4.6, hence $(\mu_2 = 4.6)$.

3. **Find the Third Centroid** $( \mu_3)$:
   - Now, calculate the distance of each remaining point to the nearest of the two selected centroids $( \mu_1)$ and $( \mu_2)$, and choose the farthest.
   - Distances to nearest centroids:
     - Distance from 0.4 to nearest of $( \mu_1)$ or $( \mu_2)$: $( \min(|0.4 - 1.7|, |0.4 - 4.6|) = \min(1.3, 4.2) = 1.3)$
     - Distance from 3.7 to nearest of $( \mu_1 )$ or $( \mu_2): ( \min(|3.7 - 1.7|, |3.7 - 4.6|) = \min(2.0, 0.9) = 0.9)$
   - The farthest point from the nearest centroid among remaining options is 0.4, hence $( \mu_3 = 0.4)$.

### Conclusion:
The three centroids chosen using the farthest-first initialization method for the given dataset $( x = [0.4, 1.7, 3.7, 4.6]^T )$ with $( K = 3)$ are:
- $( \mu_1 = 1.7)$
- $( \mu_2 = 4.6)$
- $( \mu_3 = 0.4 )$

These selections aim to maximize the spacing between initial centroids, which is beneficial for achieving a better spread in the k-means clustering process.


```python

import numpy as np # type: ignore  
  
# Data points  
x = np.array([0.4, 1.7, 3.7, 4.6])  
  
# Given initial centroid  
mu1 = 1.7  
  
# Find the farthest point from the initial centroid  
distances = np.abs(x - mu1)  
mu2 = x[np.argmax(distances)]  
  
# Find the next farthest point maximizing the distance to both mu1 and mu2  
dist_to_mu1 = np.abs(x - mu1)  
dist_to_mu2 = np.abs(x - mu2)  
min_distances = np.minimum(dist_to_mu1, dist_to_mu2)  
mu3 = x[np.argmax(min_distances)]  
  
print(f"The next two centroids are: μ2 = {mu2}, μ3 = {mu3}")
```

## 26) Which one of the following GMM densities was used to generate the data? #GMM

Let N (x|μ, Σ) denote the multivariate normal distribution with mean μ and covariance matrix Σ. In Figure 7 is given 1000 observations drawn from a density defined by a Gaussian Mixture Model (GMM) with three clusters. Each observation is colored and marked in terms of which cluster it came from in the Gaussian Mixture model. Which one of the following GMM densities was used to generate the data?

![[Pasted image 20240508221310.png]]


### A.

$p(x) = \frac{1}{2} \mathcal{N} \left( x \middle| \begin{bmatrix} -0.5 \\ -4.6 \end{bmatrix}, \begin{bmatrix} 1.7 & -1.3 \\ -1.3 & 2.1 \end{bmatrix} \right) + \frac{1}{10} \mathcal{N} \left( x \middle| \begin{bmatrix} -5.8 \\ 13.1 \end{bmatrix}, \begin{bmatrix} 2.7 & 1.0 \\ 1.0 & 1.4 \end{bmatrix} \right) + \frac{2}{5} \mathcal{N} \left( x \middle| \begin{bmatrix} 2.5 \\ 1.0 \end{bmatrix}, \begin{bmatrix} 2.1 & -1.6 \\ -1.6 & 2.4 \end{bmatrix} \right)$

### B.

$p(x) = \frac{2}{5} \mathcal{N} \left( x \middle| \begin{bmatrix} -0.5 \\ -4.6 \end{bmatrix}, \begin{bmatrix} 1.7 & -1.3 \\ -1.3 & 2.1 \end{bmatrix} \right) + \frac{1}{10} \mathcal{N} \left( x \middle| \begin{bmatrix} -5.8 \\ 13.1 \end{bmatrix}, \begin{bmatrix} 2.1 & -1.6 \\ -1.6 & 2.4 \end{bmatrix} \right) + \frac{1}{2} \mathcal{N} \left( x \middle| \begin{bmatrix} 2.5 \\ 1.0 \end{bmatrix}, \begin{bmatrix} 2.7 & 1.0 \\ 1.0 & 1.4 \end{bmatrix} \right)$

### C.

$p(x) = \frac{2}{5} \mathcal{N} \left( x \middle| \begin{bmatrix} -0.5 \\ -4.6 \end{bmatrix}, \begin{bmatrix} 2.7 & 1.0 \\ 1.0 & 1.4 \end{bmatrix} \right) + \frac{1}{10} \mathcal{N} \left( x \middle| \begin{bmatrix} -5.8 \\ 13.1 \end{bmatrix}, \begin{bmatrix} 2.1 & -1.6 \\ -1.6 & 2.4 \end{bmatrix} \right) + \frac{1}{2} \mathcal{N} \left( x \middle| \begin{bmatrix} 2.5 \\ 1.0 \end{bmatrix}, \begin{bmatrix} 1.7 & -1.3 \\ -1.3 & 2.1 \end{bmatrix} \right)$

### D.


$p(x) = \frac{1}{10} \mathcal{N} \left( x \middle| \begin{bmatrix} -0.5 \\ -4.6 \end{bmatrix}, \begin{bmatrix} 1.7 & -1.3 \\ -1.3 & 2.1 \end{bmatrix} \right) + \frac{2}{5} \mathcal{N} \left( x \middle| \begin{bmatrix} -5.8 \\ 13.1 \end{bmatrix}, \begin{bmatrix} 2.1 & -1.6 \\ -1.6 & 2.4 \end{bmatrix} \right) + \frac{1}{2} \mathcal{N} \left( x \middle| \begin{bmatrix} 2.5 \\ 1.0 \end{bmatrix}, \begin{bmatrix} 2.7 & 1.0 \\ 1.0 & 1.4 \end{bmatrix} \right)$


Remember the 1st matrix eg. $\begin{bmatrix} 2.5 \\ 1.0 \end{bmatrix}$ tells us where it is centered so we know that this is the yellow one, therefore we can exclude A & C because the diagonals are negative and the yellow is projected positively. Then we look at the mixing coefficients.

They reflect the probability or proportion of the entire dataset that is assumed to come from each component. These coefficients must sum to 1 across all components in the mixture. The value of a coefficient tells you how much of the data is expected to be generated by that particular Gaussian component.

- A higher mixing coefficient means that more of the data is expected to come from that Gaussian component.
- A lower coefficient suggests that the component contributes less to the overall data distribution.

We can see that orange has the least data so the coefficient $1/10$ belongs to it therefore option B is correct.

## 27) ROC #TODO #ROC