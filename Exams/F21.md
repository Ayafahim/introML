
## 1) Which histogram plots match which boxplots? #boxplot
![[Pasted image 20240508185706.png]]
![[Pasted image 20240508185734.png]]

From the histograms, we see that x3 (stearic) has a long right tail. For x8 (eicosenoic) more than a quarter of the observations are close to 0, which means that the first quartile must also be close to 0. Using this knowledge boxplot 2 is matched to x3 (stearic), and boxplot 4 is matched to x8 (eicosenoic).
Also you can see outliers in boxplot 2

## 2) Which one of the following matrices  is the correct empirical covariance matrix for these attributes? #Covariance

![[Pasted image 20240508193237.png]]

A. 
$$
\begin{bmatrix}
564.3 & -77.5 & 292.5 & -388.5 & 164.0 \\
-77.5 & 271.5 & -72.5 & 36.0 & -42.0 \\
292.5 & -72.5 & 392.4 & -324.8 & 248.1 \\
-388.5 & 36.0 & -324.8 & 369.9 & -241.4 \\
164.0 & -42.0 & 248.1 & -241.4 & 224.6 \\
\end{bmatrix}
$$

B.
$$
\begin{bmatrix}
-564.3 & -77.5 & 292.5 & -388.5 & 164.0 \\
-77.5 & -271.5 & -72.5 & 36.0 & -42.0 \\
292.5 & -72.5 & -392.4 & -324.8 & 248.1 \\
-388.5 & 36.0 & -324.8 & -369.9 & -241.4 \\
164.0 & -42.0 & 248.1 & -241.4 & -224.6 \\
\end{bmatrix}
$$

C.
$$
\begin{bmatrix}
224.6 & 248.1 & -42.0 & -241.4 & 164.0 \\
248.1 & 392.4 & -72.5 & -324.8 & 292.5 \\
-42.0 & -72.5 & 271.5 & 36.0 & -77.5 \\
-241.4 & -324.8 & 36.0 & 369.9 & -388.5 \\
164.0 & 292.5 & -77.5 & -388.5 & 564.3 \\
\end{bmatrix}
$$

D.
$$
\begin{bmatrix}
-224.6 & 248.1 & -42.0 & -241.4 & 164.0 \\
248.1 & -392.4 & -72.5 & -324.8 & 292.5 \\
-42.0 & -72.5 & -271.5 & 36.0 & -77.5 \\
-241.4 & -324.8 & 36.0 & -369.9 & -388.5 \\
164.0 & 292.5 & -77.5 & -388.5 & -564.3 \\
\end{bmatrix}
$$

We can rule out B & D since the diagonal is negative and that is not valid. Then you look eg. at $x_1,x_2$ in A its -77.5 which is negative and that does not match the plot since it is positively projected so it must be C.

## 3) Which one of the following statements is true? #PCA 

A Principal Component Analysis (PCA) is carried out on the Olive Oil dataset in Table 1 based on the attributes $x_1$, $x_2$, $x_3$, $x_4$ and $x_5$.

The data is standardized by (i) subtracting the mean and (ii) dividing each column by its standard deviation to obtain the standardized data matrix $\mathbf{\tilde{X}}$. A singular value decomposition is then carried out on the standardized data matrix to obtain the decomposition $\mathbf{U}\mathbf{S}\mathbf{V}^T = \mathbf{\tilde{X}}$

$$
\mathbf{V} = \begin{bmatrix}
0.48 & 0.09 & -0.57 & 0.52 & 0.42 \\
0.51 & 0.03 & -0.27 & -0.82 & 0.05 \\
-0.15 & 0.98 & 0.03 & -0.07 & 0.08 \\
-0.54 & -0.16 & -0.14 & 0.25 & 0.78 \\
0.45 & 0.01 & 0.77 & 0.05 & 0.46 \\
\end{bmatrix}
$$

$$
\mathbf{S} = \begin{bmatrix}
43.4 & 0.0 & 0.0 & 0.0 & 0.0 \\
0.0 & 23.39 & 0.0 & 0.0 & 0.0 \\
0.0 & 0.0 & 18.26 & 0.0 & 0.0 \\
0.0 & 0.0 & 0.0 & 9.34 & 0.0 \\
0.0 & 0.0 & 0.0 & 0.0 & 2.14 \\
\end{bmatrix}
$$

### Which one of the following statements is true?

A. The variance explained by the last four principal components is less than 0.3 of the total variance.

B. The variance explained by the first three principal components is greater than 0.9 of the total variance.

C. The variance explained by the first four principal components is less than 0.95 of the total variance.

D. The variance explained by the first principal component is greater than 0.715 of the total variance.

```python
import numpy as np  
  
# Singular values from the matrix S  
singular_values = np.array([43.4, 23.39, 18.26, 9.34, 2.14])  
  
# Square the singular values to get the variances  
variances = singular_values ** 2  
  
# Total variance is the sum of individual variances  
total_variance = np.sum(variances)  
  
# Proportion of variance explained by each principal component  
proportion_variance_explained = variances / total_variance  
  
# Compute cumulative variance explained by the first one, two, three, and four principal components  
cumulative_variance_one = np.sum(proportion_variance_explained[:1])  
cumulative_variance_three = np.sum(proportion_variance_explained[:3])  
cumulative_variance_four = np.sum(proportion_variance_explained[:4])  
  
# Cumulative variance explained by the last four principal components (excluding the first one)  
cumulative_variance_last_four = np.sum(proportion_variance_explained[1:])  
  
# Print the results  
print("Proportion of variance explained by each component:", proportion_variance_explained)  
print("Cumulative variance explained by one component:", cumulative_variance_one)  
print("Cumulative variance explained by three components:", cumulative_variance_three)  
print("Cumulative variance explained by four components:", cumulative_variance_four)  
print("Cumulative variance explained by the last four components:", cumulative_variance_last_four)  
  
# Evaluate the statements given in the problem  
A = cumulative_variance_last_four < 0.3  
B = cumulative_variance_three > 0.9  
C = cumulative_variance_four < 0.95  
D = cumulative_variance_one > 0.715  
  
# Print the truth value of each statement  
print("Statement A is", A)  
print("Statement B is", B)  
print("Statement C is", C)  
print("Statement D is", D)  
  
# Identify the correct statement  
statements = [A, B, C, D]  
statement_labels = ['A', 'B', 'C', 'D']  
for i, statement in enumerate(statements):  
    if statement:  
        print(f"The correct statement is: {statement_labels[i]}")
```

## 4) Consider again the PCA analysis for the Olive Oil dataset, in particular the SVD decomposition of $\mathbf{\tilde{X}}$ in Equation (1). Which one of the following statements is true? #PCA 

A. An observation with a low value of $x_1$ (palmitic), a low value of $x_2$ (palmitoleic), a high value of $x_4$ (oleic), and a low value of $x_5$ (linoleic) will typically have a negative value of the projection onto principal component number 1. (Correct Answer)

B. An observation with a high value of $x_3$ (stearic) will typically have a negative value of the projection onto principal component number 2.

C. An observation with a low value of $x_1$ (palmitic), a high value of $x_2$ (palmitoleic), and a high value of $x_4$ (oleic) will typically have a positive value of the projection onto principal component number 4.

D. An observation with a low value of $x_1$ (palmitic), a low value of $x_2$ (palmitoleic), and a high value of $x_5$ (linoleic) will typically have a negative value of the projection onto principal component number 3.

### Understanding the Components

- **Matrix $V$ in SVD (PCA)**: Matrix $V$ from the PCA (which is part of the SVD decomposition $USV^T = \tilde{X}$) contains the principal components. Each row or column of $V$ (depending on the convention used, which typically has principal components as columns for PCA) represents a principal component of the data.
- **Principal Components (Columns of $V$)**: These components show how each original variable (attributes like palmitic, palmitoleic, etc.) contribute to the new, transformed axes in the feature space created by PCA. A high positive or negative value indicates strong positive or negative correlation with that principal component.

### Steps to Solve the Problem:

1. **Examine the $V$ Matrix**: Look at the coefficients (entries) for each principal component (column in $V$). Positive coefficients indicate a positive correlation with the principal component and negative coefficients indicate a negative correlation.
2. **Consider the Sign and Magnitude of Coefficients**: The magnitude tells how much influence that variable has on the principal component. The sign (+/-) indicates the direction of the influence.
3. **Analyze Given Statements**:
   - For a statement that says "an observation with a low/high value of $x$ will have a positive/negative projection," you should:
     - **Low Value of $x$**: If $x$ has a negative coefficient in a principal component, a low value (which is numerically negative after centering and scaling) contributes positively to that component because of the multiplication of two negatives.
     - **High Value of $x$**: If $x$ has a positive coefficient in a principal component, a high value contributes positively to that component.

### Evaluating Statement A:
- **Statement A**: An observation with a low value of \( x1 \) (palmitic), a low value of \( x2 \) (palmitoleic), a high value of \( x4 \) (oleic), and a low value of \( x5 \) (linoleic) will typically have a negative value of the projection onto principal component number 1.
### Steps:
1. **Identify the First Principal Component**:
   - Look at the **first column** of the \( V \) matrix because each column corresponds to one principal component.

2. **Check the Coefficients**:
   - **For \( x1 \) and \( x2 \)**: Since you're given "low" values for these, and assuming these coefficients are positive in the first principal component, the negative signs of these "low" values (after mean subtraction and scaling) would contribute negatively to the projection.
   - **For \( x4 \)**: With a "high" value and assuming a positive coefficient, this would contribute positively.
   - **For \( x5 \)**: A "low" value here would contribute negatively if the coefficient is positive.

3. **Determine the Net Effect**:
   - If the first principal component's coefficients for \( x1 \) and \( x2 \) are positive, their "low" values contribute negatively.
   - If \( x4 \) has a positive coefficient, its "high" value contributes positively.
   - If \( x5 \) has a positive coefficient, its "low" value also contributes negatively.
   - Sum these influences (considering their signs). The predominant sign of the resulting sum will indicate whether the projection is generally positive or negative.

### Conclusion:
If the negative contributions from the "low" values of \( x1 \), \( x2 \), and \( x5 \) outweigh the positive contribution from the "high" value of \( x4 \), the overall projection onto the first principal component would be negative, making Statement A true if this is the case. However, the actual truth of the statement depends on the specific coefficients in the \( V \) matrix for the first principal component.

This approach lets you use the properties of the \( V \) matrix effectively to predict how changes in the original variables influence their representation in the transformed feature space created by PCA.

## 5) All the objects from four regions of origin are projected onto the first two principal components and visualised as a scatter plot in Figure 4. Which one of the following statements is true? #PCA

![[Pasted image 20240508203148.png]]

A Principal Component Analysis (PCA) is carried out on all the eight attributes of the Olive Oil dataset in Table 1. All the objects from four regions of origin are projected onto the first two principal components and visualised as a scatter plot in Figure 4. Which one of the following statements is true?

A. There exists a logistic regression classifier that takes the observations projected onto the first two principal components as input, which can binary classify the observations in the two regions South Apulia (y = 3) and Sicily (y = 4) with 0 error.

B. Any classifications tree using axis-aligned splits that takes the observation projected onto the first two principal components as input and binary classify the observations in the two regions South Apulia (y = 3) and Umbria (y = 9) has an error strictly greater than 0.

C. Any classification tree using axis-aligned splits that takes all eight attributes as input and binary classify the observations in the two regions South Apulia (y = 3) and Inner Sardinia (y = 5) has an error strictly greater than 0.

D. There exists a logistic regression classifier that takes all eight attributes as input, which can binary classify the observations in the two regions South Apulia (y = 3) and Umbria (y = 9) with 0 error.

### Explanation of Answers

- **Answer A** is incorrect, since the points of the two classes South Apulia (y = 3) and Sicily (y = 4) are not linearly separable in Figure 4.
- **Answer B** is incorrect, since a tree with two leafs (splitting e.g. around −1 in the projection onto the first principal component) will be able to perfectly classify the objects.
- **Answer C** is incorrect, since a classification tree is always able to obtain an error of 0 when there is no identical training object in the two classes (unless the tree complexity is limited).
- **Answer D** is correct, since the two classes South Apulia (y = 3) and Umbria (y = 9) are linearly separable in the PCA plot. Furthermore, if points are linearly separable in the projection onto the first two principal components, then they are also linearly separable in the original attribute space.

## 6) #TODO

## 7) A hierarchical clustering is applied to the 11 observations in Table 2 using maximum linkage. Which one of the dendrograms shown in Figure 5 corresponds to the distances given in Table 2? #Dendogram

Just use this script and then match it to the plots. OBS. it. might be mirrored.
```python
import numpy as np # type: ignore  
import matplotlib.pyplot as plt # type: ignore  
from scipy.cluster.hierarchy import dendrogram, linkage # type: ignore  
from scipy.spatial.distance import squareform # type: ignore  
  
correct_distance_matrix = np.array([  
    #1     #2    #3    #4     #5    #6    #7    #8    #9    #10    #11  
    [0.0, 53.8, 87.0, 67.4, 67.5, 71.2, 65.2, 117.9, 56.1, 90.3, 109.8],  
    [53.8, 0.0, 69.9, 75.5, 62.9, 58.0, 63.0, 135.0, 84.1, 107.9, 131.5],  
    [87.0, 69.9, 0.0, 49.7, 38.5, 19.3, 35.5, 91.8, 76.9, 78.7, 89.1],  
    [67.4, 75.5, 49.7, 0.0, 24.2, 47.2, 47.0, 62.3, 33.4, 37.2, 60.0],  
    [67.5, 62.9, 38.5, 24.2, 0.0, 37.7, 41.7, 79.5, 52.4, 60.2, 78.9],  
    [71.2, 58.0, 19.3, 47.2, 37.7, 0.0, 21.5, 95.6, 68.3, 78.4, 91.0],  
    [65.2, 63.0, 35.5, 47.0, 41.7, 21.5, 0.0, 96.0, 64.3, 75.5, 89.4],  
    [117.9, 135.0, 91.8, 62.3, 79.5, 95.6, 96.0, 0.0, 66.9, 44.3, 24.2],  
    [56.1, 84.1, 76.9, 33.4, 52.4, 68.3, 64.3, 66.9, 0.0, 39.2, 60.7],  
    [90.3, 107.9, 78.7, 37.2, 60.2, 78.4, 75.5, 44.3, 39.2, 0.0, 39.4],  
    [109.8, 131.5, 89.1, 60.0, 78.9, 91.0, 89.4, 24.2, 60.7, 39.4, 0.0]  
])  
correct_distance_matrix = (correct_distance_matrix + correct_distance_matrix.T) / 2  
# Set diagonal to zero  
np.fill_diagonal(correct_distance_matrix, 0)  
  
# Convert the distance matrix to a condensed form  
condensed_matrix = squareform(correct_distance_matrix)  
  
# Perform hierarchical clustering using complete linkage  
  
# TODO: If the question is minimum/single linkage use 'single' otherwise if maximum/complete linkage use 'complete' in the second parameter  
linked = linkage(condensed_matrix, 'complete')  
  
# Plot the dendrogram  
plt.figure(figsize=(10, 7))  
dendrogram(linked,  
           orientation='top',  
           labels=range(1, 12), # TODO: Remember to change the range corresponding to the size of matrix/table (if it's 9x9 then range is 1,10 if 10x10 range it 1,11)  
           distance_sort='descending',  
           show_leaf_counts=True)  
plt.title('Dendrogram for Hierarchical Clustering')  
plt.xlabel('Index of Point')  
plt.ylabel('Distance')  
plt.show()
```

## 8) To examine if observation $o_5$ may be an outlier, we will calculate the K-nearest neighbor density using only the observations and distances in Table 2. For an observation $o_i$, recall the density is computed using the set of K nearest neighbors of observation $o_i$, excluding the $i^{th}$ observation itself, $N_{X_i} (o_i, K)$, and is denoted by `density_{X_i} (o_i, K)`. What is the density for observation $o_5$ for $K = 3$ nearest neighbors? #EuclianDistances 

```python

import numpy as np # type: ignore  
import matplotlib.pyplot as plt # type: ignore  
from scipy.cluster.hierarchy import dendrogram, linkage # type: ignore  
from scipy.spatial.distance import squareform # type: ignore  
  
correct_distance_matrix = np.array([  
    #1     #2    #3    #4     #5    #6    #7    #8    #9    #10    #11  
    [0.0, 53.8, 87.0, 67.4, 67.5, 71.2, 65.2, 117.9, 56.1, 90.3, 109.8],  
    [53.8, 0.0, 69.9, 75.5, 62.9, 58.0, 63.0, 135.0, 84.1, 107.9, 131.5],  
    [87.0, 69.9, 0.0, 49.7, 38.5, 19.3, 35.5, 91.8, 76.9, 78.7, 89.1],  
    [67.4, 75.5, 49.7, 0.0, 24.2, 47.2, 47.0, 62.3, 33.4, 37.2, 60.0],  
    [67.5, 62.9, 38.5, 24.2, 0.0, 37.7, 41.7, 79.5, 52.4, 60.2, 78.9],  
    [71.2, 58.0, 19.3, 47.2, 37.7, 0.0, 21.5, 95.6, 68.3, 78.4, 91.0],  
    [65.2, 63.0, 35.5, 47.0, 41.7, 21.5, 0.0, 96.0, 64.3, 75.5, 89.4],  
    [117.9, 135.0, 91.8, 62.3, 79.5, 95.6, 96.0, 0.0, 66.9, 44.3, 24.2],  
    [56.1, 84.1, 76.9, 33.4, 52.4, 68.3, 64.3, 66.9, 0.0, 39.2, 60.7],  
    [90.3, 107.9, 78.7, 37.2, 60.2, 78.4, 75.5, 44.3, 39.2, 0.0, 39.4],  
    [109.8, 131.5, 89.1, 60.0, 78.9, 91.0, 89.4, 24.2, 60.7, 39.4, 0.0]  
])  
correct_distance_matrix = (correct_distance_matrix + correct_distance_matrix.T) / 2  
# Set diagonal to zero  
np.fill_diagonal(correct_distance_matrix, 0)  
  
# Convert the distance matrix to a condensed form  
condensed_matrix = squareform(correct_distance_matrix)  
  
# Extract distances for observation o5 (index 4 because Python uses 0-based indexing)  
distances_o5 = correct_distance_matrix[4]  
  
# Find the distances to the nearest neighbors, excluding the zero distance to itself  
# We will sort the distances and select the smallest three that are not zero  
nearest_distances = np.sort(distances_o5[distances_o5 > 0])[:3]  
  
# Calculate the average distance to the 3 nearest neighbors  
average_distance = np.mean(nearest_distances)  
  
# Calculate the density  
density_o5 = 1 / average_distance  
  
# Print the density  
print(f"The density for observation o5 with K=3 nearest neighbors is: {density_o5:.3f}")

```

## 9) What is the estimated density at $O11$ using these assumptions? #EuclianDistances 

Consider again the distances calculated from the Olive Oil dataset in Table 1 with $M = 8$ features. We wish to apply kernel density estimation for observations in the dataset. Apply kernel density estimation for the observation $o_{11}$, where only the closest two observations are used to estimate the kernel density and excluding $o_{11}$. Set the kernel width $\lambda = 20$. What is the estimated density at $o_{11}$ using these assumptions?

A. $p_\lambda(o_{11}) \approx \frac{1}{2} \cdot \frac{1}{\sqrt{(2\pi \cdot 20^2)^8}} \cdot 0.6246$ (Answer)

B. $p_\lambda(o_{11}) \approx \frac{1}{2} \cdot \frac{1}{\sqrt{(2\pi \cdot 20^2)^8}} \cdot 1.922$

C. $p_\lambda(o_{11}) \approx \frac{1}{\sqrt{(2\pi \cdot 20^2)^8}} \cdot 0.6246$

D. $p_\lambda(o_{11}) \approx \frac{1}{\sqrt{(2\pi \cdot 20^2)^8}} \cdot 1.922$


```python
import numpy as np # type: ignore

correct_distance_matrix = np.array([
    [0.0, 53.8, 87.0, 67.4, 67.5, 71.2, 65.2, 117.9, 56.1, 90.3, 109.8],
    [53.8, 0.0, 69.9, 75.5, 62.9, 58.0, 63.0, 135.0, 84.1, 107.9, 131.5],
    [87.0, 69.9, 0.0, 49.7, 38.5, 19.3, 35.5, 91.8, 76.9, 78.7, 89.1],
    [67.4, 75.5, 49.7, 0.0, 24.2, 47.2, 47.0, 62.3, 33.4, 37.2, 60.0],
    [67.5, 62.9, 38.5, 24.2, 0.0, 37.7, 41.7, 79.5, 52.4, 60.2, 78.9],
    [71.2, 58.0, 19.3, 47.2, 37.7, 0.0, 21.5, 95.6, 68.3, 78.4, 91.0],
    [65.2, 63.0, 35.5, 47.0, 41.7, 21.5, 0.0, 96.0, 64.3, 75.5, 89.4],
    [117.9, 135.0, 91.8, 62.3, 79.5, 95.6, 96.0, 0.0, 66.9, 44.3, 24.2],
    [56.1, 84.1, 76.9, 33.4, 52.4, 68.3, 64.3, 66.9, 0.0, 39.2, 60.7],
    [90.3, 107.9, 78.7, 37.2, 60.2, 78.4, 75.5, 44.3, 39.2, 0.0, 39.4],
    [109.8, 131.5, 89.1, 60.0, 78.9, 91.0, 89.4, 24.2, 60.7, 39.4, 0.0]
])

# Distances for observation o11 from the distance matrix
distances_o11 = correct_distance_matrix[10]  # Index 10 corresponds to o11

# Identify the two smallest distances (excluding the distance to itself, which is 0)
sorted_indices = np.argsort(distances_o11)
closest_two_indices = sorted_indices[1:3]  # Skip the first one as it is the distance to itself

# Distances of the two closest observations to o11
closest_distances = distances_o11[closest_two_indices]

# Number of features M
M = 8

# Bandwidth lambda
lambda_ = 20

# Gaussian kernel density estimation
kde_estimate = (1 / (2 * np.sqrt((2 * np.pi * lambda_**2)**M))) * np.sum(np.exp(-closest_distances**2 / (2 * lambda_**2)))

print(kde_estimate, closest_distances)


# TODO CHECK THESE OPTIONS FROM THE ANSWERS
lambda_ = 20
M = 8
two_pi_lambda_squared = (2 * np.pi * lambda_**2)**M

# Calculate the denominator for the density estimations
denominator = np.sqrt(two_pi_lambda_squared)

# Compute the estimates for each option
option_A = 0.5 / denominator * 0.6246
option_B = 0.5 / denominator * 1.922
option_C = 1 / denominator * 0.6246
option_D = 1 / denominator * 1.922

print("A: ", option_A)
print("B: ", option_B)
print("C: ", option_C)
print("D: ", option_D)
```

----------------------------
```
#7.826826302434703e-15 [24.2 39.4]
#A:  7.827310200532026e-15
#B:  2.4085959342655385e-14
#C:  1.5654620401064053e-14
#D:  4.817191868531077e-14
```



## 10) Now, we consider the binarized version  of the Olive Oil dataset in Table 3. According to this  dataset, what is the probability that a sample comes  from the region Calabria given that we in that sample observe that the palmitic content is below the median and that the arachidic content is above the median? #BinaryVersion

A. $p(C_2 \mid f_1 = 0, f_6 = 1) = \frac{5}{11}$

B. $p(C_2 \mid f_1 = 0, f_6 = 1) = \frac{4}{7}$

C. $p(C_2 \mid f_1 = 0, f_6 = 1) = \frac{5}{7}$ (Correct)

D. $p(C_2 \mid f_1 = 0, f_6 = 1) = 1$


```python
import numpy as np # type: ignore  
  
# Define the binarized dataset with corrected understanding or labels if necessary  
data = np.array([  
    [0, 0, 0, 1, 0, 0, 0, 1],  # o1 -> C1  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o2 -> C1  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o3 -> C2  
    [0, 1, 0, 0, 0, 1, 0, 1],  # o4 -> C2  
    [0, 0, 0, 0, 0, 1, 0, 1],  # o5 -> C2  
    [0, 0, 1, 0, 1, 1, 0, 1],  # o6 -> C2  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o7 -> C2  
    [1, 1, 0, 0, 0, 0, 1, 1],  # o8 -> C3  
    [0, 1, 0, 0, 0, 0, 0, 1],  # o9 -> C3  
    [0, 1, 0, 0, 0, 1, 0, 1],  # o10 -> C3  
    [1, 1, 0, 0, 0, 0, 0, 0],  # o11 -> C3  
])  
  
# Define the class labels for the data  
classes = np.array([1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3])  # Classes corresponding to each observation  
  
# Filter the data where f1 = 0 and f6 = 1  
mask_f1_0_f6_1 = (data[:, 0] == 0) & (data[:, 5] == 1)  
  
# Filtered class labels for the condition f1 = 0 and f6 = 1  
filtered_classes = classes[mask_f1_0_f6_1]  
  
# Count the occurrences of class C2 in the filtered dataset  
count_C2 = np.sum(filtered_classes == 2)  
  
# Total number of samples meeting the condition f1 = 0 and f6 = 1  
total_filtered = len(filtered_classes)  
  
# Calculate the conditional probability p(C2 | f1 = 0, f6 = 1)  
conditional_probability = count_C2 / total_filtered if total_filtered > 0 else 0  
print(conditional_probability, count_C2, total_filtered)
```
----
```
0.7142857142857143 5 7
```

## 11) Consider the observations in Table 3. We consider these as 8-dimensional binary vectors and wish to compute the pairwise similarity. Which one of the following statements is true? #jaccard #BinaryVersion 

```python
import numpy as np  
  
def cosine_similarity(vec1, vec2):  
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))  
  
  
def simple_matching_coefficient(vec1, vec2):  
    return sum(1 for i, j in zip(vec1, vec2) if i == j) / len(vec1)  
  
  
def jaccard_similarity(vec1, vec2):  
    intersection = sum(1 for i, j in zip(vec1, vec2) if i == j and i == 1)  
    union = sum(1 for i, j in zip(vec1, vec2) if i == 1 or j == 1)  
    return intersection / union  
  
  
# Observations from Table 4 as 5-dimensional binary vectors  
o1 = np.array([0, 0, 0, 1, 0, 0, 0, 1])  
o2 = np.array([0, 0, 1, 0, 0, 1, 0, 1])  
o3 = np.array([0, 0, 1, 0, 0, 1, 0, 1])  
o4 = np.array([0, 1, 0, 0, 0, 1, 0, 1])  
  
# Calculate similarities and coefficients  
print(simple_matching_coefficient(o2, o4))  
print(cosine_similarity(o1, o2))  
print(simple_matching_coefficient(o3, o4))  
print(jaccard_similarity(o2, o4))
```

## 12) Consider again the binary data presented in Table 3 with three classes. We will use Hunt’s algorithm to construct a classification tree using the Gini impurity measure. Suppose that the data in Table 3 is at the root node, and a binary split is made based on two different values of $f_2$. What is the impurity gain of this split? #HuntsAlgo 

```python
from fractions import Fraction  
import numpy as np  # type: ignore  
  
# Data from the table, with class labels assigned based on the provided information  
data = np.array([  
    [0, 0, 0, 1, 0, 0, 0, 1],  # o1 -> C1  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o2 -> C1  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o3 -> C2  
    [0, 1, 0, 0, 0, 1, 0, 1],  # o4 -> C2  
    [0, 0, 0, 0, 0, 1, 0, 1],  # o5 -> C2  
    [0, 0, 1, 0, 1, 1, 0, 1],  # o6 -> C2  
    [0, 0, 1, 0, 0, 1, 0, 1],  # o7 -> C2  
    [1, 1, 0, 0, 0, 0, 1, 1],  # o8 -> C3  
    [0, 1, 0, 0, 0, 0, 0, 1],  # o9 -> C3  
    [0, 1, 0, 0, 0, 1, 0, 1],  # o10 -> C3  
    [1, 1, 0, 0, 0, 0, 0, 0],  # o11 -> C3  
])  
  
# Class labels as described  
class_labels = np.array([1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3])  
  
  
# Gini impurity calculation  
def gini_impurity(classes):  
    _, counts = np.unique(classes, return_counts=True)  
    probabilities = counts / counts.sum()  
    return 1 - np.sum(probabilities ** 2)  
  
  
# Compute Gini impurity for the root node  
root_impurity = gini_impurity(class_labels)  
  
# Compute Gini impurities for the split on f2  
mask_f2_0 = data[:, 1] == 0  
mask_f2_1 = data[:, 1] == 1  
  
gini_f2_0 = gini_impurity(class_labels[mask_f2_0])  
gini_f2_1 = gini_impurity(class_labels[mask_f2_1])  
  
# Calculate the proportions for weighted average  
prop_f2_0 = mask_f2_0.sum() / len(class_labels)  
prop_f2_1 = mask_f2_1.sum() / len(class_labels)  
  
# Weighted Gini impurity after the split  
weighted_gini = prop_f2_0 * gini_f2_0 + prop_f2_1 * gini_f2_1  
  
# Impurity gain from the split  
gini_gain = root_impurity - weighted_gini  
  
print(root_impurity, gini_f2_0, gini_f2_1, weighted_gini, gini_gain)  
fraction = Fraction(gini_gain).limit_denominator()  
  
# Print the probability and the fraction  
print("Probability:", gini_gain)  
print("Fraction:", fraction)
```

## 13) 

## 17) How many parameters has to be trained to fit the neural network? #ANN

An artificial neural network (ANN) trained on the Olive Oil dataset described in Table 1  will be used to predict the region of origin in Italy y as a multi-class classification problem based on all of the attributes x1, . . . , x8. The neural network has a single hidden layer containing nh = 50 units that uses a sigmoid non-linear activation function. The output layer uses a softmax activation function as described in the lecture notes, Section 15.3.2. How many parameters has to be trained to fit the neural network?
### Step-by-Step Calculation:

1. **Architecture Details**:
   - **Input Features (M)**: 8 (x1 through x8)
   - **Hidden Units (nh)**: 50
   - **Output Classes (C)**: 9 (one for each class)

2. **Parameters between Input Layer and Hidden Layer**:
   - Total parameters for the hidden layer = $(M + 1) \times nh = (8 + 1) \times 50 = 9 \times 50 = 450$

3. **Parameters between Hidden Layer and Output Layer**:
   - Total parameters for the output layer = $(nh + 1) \times C = (50 + 1) \times 9 = 51 \times 9 = 459$

4. **Total Parameters in the Network**:
   - Total parameters = $450 + 459 = 909$

### Conclusion:
The total number of parameters that need to be trained in this neural network is $909$.


