
## 1) Which histograms x1, x2, x3, x4 match which boxplots?


## 2) PCA - 

```python
import numpy as np  
  
# Singular values from the matrix S  
singular_values = np.array([149, 118, 53, 42, 3])  
  
# Square the singular values to get the variances  
variances = singular_values**2  
  
# Total variance is the sum of individual variances  
total_variance = np.sum(variances)  
  
# Proportion of variance explained by each principal component  
proportion_variance_explained = variances / total_variance  
  
# Compute cumulative variance explained by the first two and three principal components  
cumulative_variance_two = np.sum(proportion_variance_explained[:2])  
cumulative_variance_three = np.sum(proportion_variance_explained[:3])  
  
# Print the results  
print("Proportion of variance explained by each component:", proportion_variance_explained)  
print("Cumulative variance explained by the first two components:", cumulative_variance_two)  
print("Cumulative variance explained by the first three components:", cumulative_variance_three)  
  
# Check the statements given in the problem  
A = np.sum(proportion_variance_explained[-3:]) < 0.10  
B = proportion_variance_explained[0] > 0.60  
C = np.sum(proportion_variance_explained[-2:]) < 0.04  
D = cumulative_variance_two > 0.85  
  
# Print the truth value of each statement  
print("Statement A is", A)  
print("Statement B is", B)  
print("Statement C is", C)  
print("Statement D is", D)  
  
# Identify the correct statement  
statements = [A, B, C, D]  
statement_labels = ['A', 'B', 'C', 'D']  
for i, statement in enumerate(statements):  
    if statement:  
        print(f"The correct statement is: {statement_labels[i]}")
```


## 3) plot of each observation plotted onto the two first principal directions given in Equation...Which of the following statements best describes the development of  the measurements? 

Info from last 2 problems:
![[Screenshot 2024-04-18 at 17.30.27.png]]
![[Screenshot 2024-04-18 at 17.30.42.png]]


![[Pasted image 20240418173502.png]]

The matrix `S` is a diagonal matrix containing the singular values of the PCA, and the matrix `V` contains the principal directions (eigenvectors). We have to only look at the mesaurements from start to end which goes from 1 to -3 on the y-axis. The operation `-3v2 - 1v2` (it is -1 because we compute difference) is linear combination of the second column of the `V` matrix, which corresponds to the second principal component. 

```python
import numpy as np  
  
# V matrix from the second image  
V = np.array([  
    [-0.3, -0.5,  0.7,  0.2,  0.2],  
    [-0.4,  0.6, -0.0,  0.2,  0.7],  
    [-0.4, -0.4, -0.7,  0.4, -0.0],  
    [-0.6, -0.1, -0.1, -0.8,  0.1],  
    [-0.5,  0.4,  0.2,  0.2, -0.7]  
])  
  
# Extracting the second principal component (v2)  
v2 = V[:, 1]  # This is the second column of V  
  
# Computing -3v2 - 1v2  
result_vector = -3 * v2 - 1 * v2  
  
# Display the result  
print(result_vector)


#result
#[ 2.  -2.4  1.6  0.4 -1.6]
# 2 = x1, -2.4 = x2, etc...
#If negative coefficent negative impact, otherwise positive impact

# We see the temperature goes up, the humidity drops and the light goes up, therefore option A is correct.

```

#EuclianDistances 
## 4) Consider the distances in Table 2. The  class labels C1, C2 (corresponding to {o1, o2, o3, o4, o5} and {o6, o7, o8, o9}) will be predicted using a k-nearest neighbour classifier based on the distances given in Table 2.Suppose we use leave-one-out cross validation  (i.e. the observation that is being predicted is left out) and a 3-nearest neighbour classifier (i.e. k = 3). What is the error rate computed for all N = 9 observations?

![[Pasted image 20240418203729.png]]

## Solution 
The error rate is `2/9` because we have to find the 3 closest values to the values in first column. if majority of the values/neighbours are in the same class it is the correctly classified otherwise its wrong. 
Consider e.g. $O_1$, its closest neighbours are $O_3, O_5, O_6$  where 2/3 are in same class there for it is correctly classified. Whereas, the closest neighbours are $O_7, O_8, O_3$ where 2/3 are in the other class, therefore it is wrongly classified.


## 5) Consider the distances in Table 2 and  suppose we wish to apply mixture modelling and we use the normal density as the mixture distributions:

$$
p(x | \mu, \sigma) = \mathcal{N}(x; \mu, \sigma) = \left( \frac{1}{(2\pi\sigma^2)^{\frac{M}{2}}} \right) e^{-\frac{\|x-\mu\|^2}{2\sigma^2}}.
$$


![[Pasted image 20240418210105.png]]
## Solution
Options A and B are not properly normalized by the number of mixture components. Option C does not use the squared distances. Accordingly option D is the correct answer.

## 6) What is the a.r.d. of observation o9 using K = 2 nearest neighbours?

We wish to compute the average relative KNN density (a.r.d) of observation $O_9$ from the Occupancy dataset described in Table 1 using the distances given in Table 2. Letting $d(x, y)$ denote the Euclidian distance metric the a.r.d. is defined as:

$$
\text{density}(x, K) = \frac{1}{K} \sum_{y \in N(x, K)} d(\bar{x}, y)
$$

$$
\text{a.r.d}(x, K) = \frac{1}{K} \sum_{z \in N(x, K)} \frac{\text{density}(x, K)}{\text{density}(z, K)},
$$

where \( N(x, K) \) is the set of \( K \)-nearest neighbors of \( x \).

## Solution
The nearest neighbour of $O_9$ is $O_6$, $O_8$ and the nearest neighbours of $O_6$ is $O_2$, $O_9$ and for $O_8$ it is $O_7$, $O_9$. The densities are:
$density(O_9, K = 2) = 0.746268656716$
$density(O_6, K = 2) = 0.505050505051$ 
$density(O_8, K = 2) = 0.724637681159$
so: 

$$
\text{a.r.d}(o_g, K = 2) = \frac{0.7463}{\frac{1}{2}(0.5051 + 0.7246)} = 1.2138
$$


```python
import numpy as np  
  
def calculate_density(nearest_distances, K):  
    """  
    Calculate the density for a given observation based on its K-nearest neighbors.  
    Parameters:    - nearest_distances: A list of distances from the observation to its K-nearest neighbors.    - K: The number of nearest neighbors to consider.  
    Returns:    - The density value for the observation.    """    # Ensure the list is sorted and take the K smallest distances  
    if len(nearest_distances) > K:  
        nearest_distances = sorted(nearest_distances)[:K]  
    return 1 / np.mean(nearest_distances)  
  
def calculate_ard(nearest_distances_o, nearest_distances_neighbors, K):  
    """  
    Calculate the average relative density (a.r.d.) for a given observation.  
    Parameters:    - nearest_distances_o: A list of distances from the observation to its K-nearest neighbors.    - nearest_distances_neighbors: A list of lists, each containing distances from each neighbor to its K-nearest neighbors.    - K: The number of nearest neighbors to consider.  
    Returns:    - The average relative density (a.r.d.) for the observation.    """    density_o = calculate_density(nearest_distances_o, K)  
    densities_neighbors = [calculate_density(distances, K) for distances in nearest_distances_neighbors]  
    return density_o / np.mean(densities_neighbors)  
  
# Example usage  

# Remember whatever K is, is the amount of neighbours to find. fo remember to add them.
K = 2  
nearest_distances_o9 = [1.95, 0.73]  # Distances to the two nearest neighbors o6 and o8  
nearest_distances_o6 = [1.95, 2.01]  # Distances from o6 to its two nearest neighbors  
nearest_distances_o8 = [0.73, 2.03]  # Distances from o8 to its two nearest neighbors  
  
# Calculate a.r.d. for o9  
ard_o9 = calculate_ard(nearest_distances_o9, [nearest_distances_o6, nearest_distances_o8], K)  
  
# Print the result  
print(f"Average Relative Density (a.r.d.) of o9: {ard_o9:.4f}")
```

## 7) A hierarchical clustering is applied to the 9 observations in Table 2 using minimum linkage.  Which of the dendrograms shown in Figure 4 corresponds to the clustering? #TODO

![[Pasted image 20240420141416.png]]

### Solution
The correct answer is D, dendrogram 4. $O_8$ and $O_9$ are grouped together in all diagrams. Since the distance from $O_6$ to $O_8$ is lower than the distance from $O_7$ to $O_6$ then $O_6$ should link to $O_8$, $O_9$ before $O_7$. This allows us to rule out dendrogram 1 and dendrogram 2. Finally, $O_3$ and $O_1$ should clearly link together allowing us to rule out dendrogram 3. This leaves only option D.



## 8) What can be concluded about the Cosine similarity of these two observations? (Hint: recall for vectors $x, y$ that $||x - y||^2_2 = ||x||^2_2 + ||y||^2_2 - 2xᵀy$)

In Table 2 is given the pairwise euclidian distances between 9 observations from the Occupancy dataset of Table 1. Suppose the Euclidian norm of observations o2 and o3 is:

$$||o_2|| = \sqrt{\sum_{k=1}^{M} x_{1k}^2} = 3.04, \quad ||o_3|| = \sqrt{\sum_{k=1}^{M} x_{2k}^2} = 1.5$$


To find the cosine similarity between observations $o_2$ and $o_3$, we can use the relationship given in the hint and the norm of the observations:

$$ ||o_2 - o_3||^2_2 = ||o_2||^2_2 + ||o_3||^2_2 - 2 \cdot o_2^To_3 $$

Cosine similarity is defined as:

$$ \text{cosine similarity} = \frac{o_2^To_3}{||o_2|| \cdot ||o_3||} $$

From the provided hint and equation, we can isolate $o_2^To_3$:

$$ 2 \cdot o_2^To_3 = ||o_2||^2_2 + ||o_3||^2_2 - ||o_2 - o_3||^2_2 $$

Given:

$$ ||o_2|| = 3.04 $$
$$ ||o_3|| = 1.5 $$

And from the Table 2 (not shown in the current context, but given earlier), we know the Euclidean distance between $o_2$ and $o_3$:

$$ ||o_2 - o_3|| = 4.40 $$

Now we calculate:

$$ ||o_2||^2_2 = (3.04)^2 = 9.2416 $$
$$ ||o_3||^2_2 = (1.5)^2 = 2.25 $$
$$ ||o_2 - o_3||^2_2 = (4.40)^2 = 19.36 $$

Now we plug these into the equation:

$$ 2 \cdot o_2^To_3 = 9.2416 + 2.25 - 19.36 $$

$$ 2 \cdot o_2^To_3 = 11.4916 - 19.36 $$

$$ 2 \cdot o_2^To_3 = -7.8684 $$

$$ o_2^To_3 = \frac{-7.8684}{2} $$

$$ o_2^To_3 = -3.9342 $$

Then, we calculate the cosine similarity:

$$ \text{cosine similarity} = \frac{-3.9342}{3.04 \cdot 1.5} $$

$$ \text{cosine similarity} = \frac{-3.9342}{4.56} $$

$$ \text{cosine similarity} = -0.8628 $$

Therefore, the correct answer is:

D. cos(o2, o3) ≈ −0.8628

### Script 
```python
import numpy as np  
  
  
def cosine_similarity(norm_o1, norm_o2, euclidean_distance):  
    """  
    Calculate cosine similarity between two observations.  
    Parameters:    - norm_o1: Euclidean norm of the first observation    - norm_o2: Euclidean norm of the second observation    - euclidean_distance: Euclidean distance between the two observations  
    Returns:    - Cosine similarity value    """    # Calculate the squared norms  
    squared_norm_o1 = norm_o1 ** 2  
    squared_norm_o2 = norm_o2 ** 2  
  
    # Calculate the squared Euclidean distance  
    squared_euclidean_distance = euclidean_distance ** 2  
  
    # Calculate the dot product from the norms and Euclidean distance  
    dot_product = (squared_norm_o1 + squared_norm_o2 - squared_euclidean_distance) / 2  
  
    # Calculate the cosine similarity  
    cosine_sim = dot_product / (norm_o1 * norm_o2)  
  
    return cosine_sim  
  
  
# Example usage from task:  
norm_o2 = 3.04  
norm_o3 = 1.5  
euclidean_distance_o2_o3 = 4.40 ## From the table   
# Calculate cosine similarity  
cos_sim = cosine_similarity(norm_o2, norm_o3, euclidean_distance_o2_o3)  
  
print(f"The cosine similarity is approximately {cos_sim:.4f}")
```


## 9) What will Hunt’s algorithm do if classification error is used as impurity measure?
![[Pasted image 20240420141911.png]]

To determine which split Hunt's algorithm would select, we need to calculate the classification error for each potential split and then choose the split that provides the greatest purity gain \( \Delta \). The classification error for a split is calculated by finding the proportion of the majority class within each partition of the split and then subtracting from 1.

The classification error for each node is:

$$
\text{Classification Error} = 1 - \max\left(\frac{\text{Count of Majority Class}}{\text{Total Count}}\right)
$$

To calculate the purity gain \( \Delta \) for a split, we need to calculate the weighted sum of the classification errors for the child nodes and subtract it from the classification error before the split (the parent node).

The dataset before any splits has 129 observations unoccupied (\( y = 0 \)) and 169 observations occupied (\( y = 1 \)). The classification error of the parent node is:

$$
\text{Parent Error} = 1 - \max\left(\frac{129}{129 + 169}, \frac{169}{129 + 169}\right)
$$

Next, we'll calculate the classification error for each split. Here's how to calculate it for Split 1:

For Split 1:

- Node 1 (Temperature < 20): Classification error is $1 - \frac{45}{45 + 1}$
- Node 2 (20 ≤ Temperature ≤ 22): Classification error is $1 - \frac{66}{47 + 66}$
- Node 3 (Temperature > 22): Classification error is $1 - \frac{33}{8 + 33}$

We then weigh these classification errors by the number of observations in each node to get the weighted classification error for Split 1. Finally, we do the same calculations for Splits 2 and 3.

### script

```python
def calculate_classification_error(class_counts):  
    # Calculate the classification error for a node  
    total = sum(class_counts)  
    return 1 - max(class_counts) / total if total > 0 else 0  
  
def calculate_purity_gain(I0, split, n):  
    # Calculate purity gain for a given split  
    weighted_error = sum((count / n) * calculate_classification_error((observed, count - observed))  
                         for observed, count in split)  
    return I0 - weighted_error  
  
# Initial impurity (classification error) for the root node  
I0 = 1 - 1/2  # For a perfectly balanced binary classification  
  
# Number of observations  
n = 200  
  
# Define the splits (occupied, total) from your teacher's solution  
splits_info = {  
    'Split 1': [(45, 46), (66, 113), (33, 41)],  # Split 1: (occupied, total)  
    'Split 2': [(20, 76), (47, 63), (33, 41)],   # Split 2  
    'Split 3': [(0, 25), (23, 78), (77, 97)]     # Split 3  
}  
  
# Calculate purity gains for each split and find the best split  
purity_gains = {split_label: calculate_purity_gain(I0, split, n) for split_label, split in splits_info.items()}  
best_split = max(purity_gains, key=purity_gains.get)  
best_purity_gain = purity_gains[best_split]  
  
# Output the results and the chosen split  
for split_label, purity_gain in purity_gains.items():  
    print(f"Purity gain for {split_label}: {purity_gain:.3f}")  
    if split_label == best_split:  
        print(f"{split_label} is chosen by Hunt's algorithm with a purity gain of {purity_gain:.3f}\n")  
  
# Print comparison between splits  
for split_label, purity_gain in purity_gains.items():  
    if split_label != best_split:  
        print(f"{best_split} is selected over {split_label} because {best_purity_gain:.4f} > {purity_gain:.4f}")
```

## 10) Consider the N = 11 observations from Table 2 and assume the data has been processed to the 11 × 5 binary matrix shown in Table 4. Suppose we consider the first three features f1, f2, f3 and train a Na ̈ıve-Bayes classifier to distinguish between unoccupied and occupied rooms y = 0 and y = 1 based on only these three features. If an observation has f1 = 0, f2 = 1, f3 = 1, what is the probability that the room is occupied, y = 1, according to the Naive-Bayes classifier?

```python
def calculate_naive_bayes(table, f1, f2, f3):
    # Count the number of instances for each class y=0 and y=1
    count_y0 = sum(1 for row in table if row[-1] == 0)
    count_y1 = sum(1 for row in table if row[-1] == 1)

    # Calculate prior probabilities
    prior_y0 = count_y0 / len(table)
    prior_y1 = count_y1 / len(table)

    # Calculate likelihoods for each feature given the class
    likelihood_f1_y0 = sum(1 for row in table if row[0] == f1 and row[-1] == 0) / count_y0
    likelihood_f2_y0 = sum(1 for row in table if row[1] == f2 and row[-1] == 0) / count_y0
    likelihood_f3_y0 = sum(1 for row in table if row[2] == f3 and row[-1] == 0) / count_y0

    likelihood_f1_y1 = sum(1 for row in table if row[0] == f1 and row[-1] == 1) / count_y1
    likelihood_f2_y1 = sum(1 for row in table if row[1] == f2 and row[-1] == 1) / count_y1
    likelihood_f3_y1 = sum(1 for row in table if row[2] == f3 and row[-1] == 1) / count_y1

    # Calculate the unnormalized posteriors for y=0 and y=1
    unnormalized_posterior_y0 = likelihood_f1_y0 * likelihood_f2_y0 * likelihood_f3_y0 * prior_y0
    unnormalized_posterior_y1 = likelihood_f1_y1 * likelihood_f2_y1 * likelihood_f3_y1 * prior_y1

    # Normalize the posteriors so that they sum to 1
    sum_posteriors = unnormalized_posterior_y0 + unnormalized_posterior_y1
    if sum_posteriors == 0:
        return None  # To avoid division by zero if no evidence supports either class
    posterior_y1 = unnormalized_posterior_y1 / sum_posteriors

    return posterior_y1

# The data from Table 4 converted into a list of lists
# The last element in each sub-list is the class label y, derived from the color coding in the image
table_4_data = [
    [0, 1, 1, 0, 1, 0],  # o1
    [0, 0, 1, 0, 0, 0],  # o2
    [1, 0, 0, 0, 1, 0],  # o3
    [1, 0, 0, 1, 1, 0],  # o4
    [1, 0, 0, 1, 0, 0],  # o5
    [1, 1, 0, 1, 1, 0],  # o6
    [1, 0, 1, 1, 0, 1],  # o7
    [1, 0, 1, 1, 1, 1],  # o8
    [0, 1, 1, 1, 1, 1],  # o9
    [1, 0, 1, 0, 0, 1],  # o10
    [0, 1, 1, 0, 0, 1]   # o11
]

# Example observation from the question
f1_observation = 0
f2_observation = 1
f3_observation = 1

# Calculate the probability
probability = calculate_naive_bayes(table_4_data, f1_observation, f2_observation, f3_observation)
probability
```

## 11) Suppose we consider the binary matrix in Table 4 as a market-basket problem consisting of N = 11 ”transactions” o1, . . . , o11 and M = 5 ”items” f1, . . . , f5. Which of the following options represents all itemsets with support greater than 0.32?

```python
def find_itemsets_with_support(table, num_items, min_support_ratio):
    """
    Find all itemsets (single items and pairs) that occur with at least a given support ratio.

    :param table: List of transactions, where each transaction is represented by a list of item flags (0 or 1)
    :param num_items: The number of items M
    :param min_support_ratio: The minimum support ratio
    :return: A list of itemsets that meet the minimum support ratio
    """
    itemsets = []
    num_transactions = len(table)
    min_support_count = min_support_ratio * num_transactions
    
    # Check support for individual items
    for item in range(num_items):
        support_count = sum(transaction[item] for transaction in table)
        if support_count >= min_support_count:
            itemsets.append({f'f{item+1}'})
    
    # Check support for pairs of items
    for item1 in range(num_items):
        for item2 in range(item1+1, num_items):
            support_count = sum(transaction[item1] and transaction[item2] for transaction in table)
            if support_count >= min_support_count:
                itemsets.append({f'f{item1+1}', f'f{item2+1}'})
    
    return itemsets

# M = 5 items, with support greater than 0.32
M = 5
support_ratio = 0.32

# Calculate itemsets with support greater than 0.32, including item pairs
itemsets_with_support = find_itemsets_with_support(table_4_data, M, support_ratio)
itemsets_with_support
```

## 12) We again consider the binary matrix of Table 4 as a market-basket problem consisting of  N = 11 ”transactions” o1, . . . , o11 and M = 5 ”items” f1, . . . , f5. Which of the following rules has the highest confidence?

```python
def calculate_rule_confidence(table, rule_antecedent, rule_consequent):
    """
    Calculate the confidence of a rule in the format antecedent -> consequent.

    :param table: List of transactions, where each transaction is represented by a list of item flags (0 or 1)
    :param rule_antecedent: Tuple representing the antecedent of the rule (e.g., ('f1', 'f5'))
    :param rule_consequent: Tuple representing the consequent of the rule (e.g., ('f4',))
    :return: Confidence of the rule
    """
    num_transactions = len(table)
    antecedent_count = 0
    both_count = 0
    
    # Convert feature names to indices
    antecedent_indices = [int(item[1])-1 for item in rule_antecedent]
    consequent_indices = [int(item[1])-1 for item in rule_consequent]
    
    # Count the occurrences of the antecedent and both antecedent and consequent
    for transaction in table:
        if all(transaction[index] for index in antecedent_indices):
            antecedent_count += 1
            if all(transaction[index] for index in consequent_indices):
                both_count += 1
    
    # Calculate confidence
    confidence = both_count / antecedent_count if antecedent_count else 0
    return confidence

# Rules from the question
rules = [
    (('f3', 'f4'), ('f5',)),
    (('f1', 'f5'), ('f4',)),
    (('f1', 'f4'), ('f5',)),
    (('f2', 'f4'), ('f1',)),
]

# Calculate confidence for each rule
rule_confidences = {rule: calculate_rule_confidence(table_4_data, *rule) for rule in rules}
rule_confidences
```

## 13) We consider the N = 11 observations from Table 4 as 5-dimensional binary vectors. Which one of the following statements is true regarding the Jaccard/cosine similarity and the simple matching coefficient?

```python
mport numpy as np

def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

def simple_matching_coefficient(vec1, vec2):
    return sum(1 for i, j in zip(vec1, vec2) if i == j) / len(vec1)

def jaccard_similarity(vec1, vec2):
    intersection = sum(1 for i, j in zip(vec1, vec2) if i == j and i == 1)
    union = sum(1 for i, j in zip(vec1, vec2) if i == 1 or j == 1)
    return intersection / union

# Observations from Table 4 as 5-dimensional binary vectors
o1 = np.array([0, 1, 1, 0, 1])
o2 = np.array([0, 0, 1, 0, 0])
o3 = np.array([1, 0, 0, 0, 1])

# Calculate similarities and coefficients
cos_o1_o2 = cosine_similarity(o1, o2)
smc_o1_o2 = simple_matching_coefficient(o1, o2)
cos_o1_o3 = cosine_similarity(o1, o3)
j_o1_o3 = jaccard_similarity(o1, o3)
cos_o1_o2, smc_o1_o2, cos_o1_o3, j_o1_o3
```


## 14) Consider the Occupancy dataset of Table 1 and suppose we only consider the first four features x1, x2, x3, x4. Suppose we wish to examine which subset of these features can be expected to give the optimal generalization error. In Table 5 is shown how different combinations of features give rise to different error rates on a training and a test set for a classifier. Which one of the following statements is true?

```python
def calculate_humidity_probability(conditional_probs):
    """
    Calculate the probability that a room is humid given that it is occupied.

    Args:
    conditional_probs (dict): A dictionary with the given conditional probabilities.

    Returns:
    float: The calculated probability.
    """
    # P(g2 = 1 | y = 1) = P(g1 = 0, g2 = 1 | y = 1) + P(g1 = 1, g2 = 1 | y = 1)
    probability_humid_occupied = (conditional_probs.get('P(g1=0, g2=1 | y=1)') +
                                  conditional_probs.get('P(g1=1, g2=1 | y=1)'))
    return probability_humid_occupied

# Given conditional probabilities
given_probs = {
    'P(g1=0, g2=1 | y=1)': 0.03,
    'P(g1=1, g2=1 | y=1)': 0.50
}

# Calculate the probability
humidity_probability = calculate_humidity_probability(given_probs)
humidity_probability
```


## 18) 

### Overview of Norms

- **Infinity Norm ($\|x\|_\infty$)**: The infinity norm of a vector x, which is the maximum absolute value of its components. This norm measures the "worst-case" or maximum displacement along any coordinate axis.
  
  **Example**: For x = (1, -3), $\|x\|_\infty = \max(|1|, |-3|) = 3$.

- **Manhattan Norm ($\|x\|_1$)**: The Manhattan (or L1) norm of a vector x, which is the sum of the absolute values of its components. This norm measures the total distance traveled along the axes from the origin to the point.
  
  **Example**: For x = (1, -3), $\|x\|_1 = |1| + |-3| = 4$.

- **Euclidean Norm ($\|x\|_2$)**: The Euclidean (or L2) norm of a vector x, which is the square root of the sum of the squares of its components. It represents the straight-line distance from the origin to the point.
  
  **Example**: For x = (1, -3), $\|x\|_2 = \sqrt{1^2 + (-3)^2} = \sqrt{10}$.

### Explanation of Options

Here are detailed explanations and simple examples for each option's rules applied to decision tree nodes A, B, and C:

#### Option A
- **A**: $\|x\|_\infty < \frac{1}{4}$
  - **Meaning**: This rule checks if the maximum absolute value of either coordinate of x is less than 0.25.
  - **Example**: For x = (0.1, -0.2), $\|x\|_\infty = \max(0.1, 0.2) = 0.2 < 0.25$ so the rule is true.

- **B**: $\|x - [-1]\|_1 > 2$
  - **Meaning**: This measures the Manhattan distance from the point x to the point [-1, -1], checking if it is greater than 2.
  - **Example**: For x = (0, 0), $\|x - [-1, -1]\|_1 = |0+1| + |0+1| = 2$, so the rule is false.

- **C**: $\|x - [0, 1]\|_2 < 1$
  - **Meaning**: This measures the Euclidean distance from the point x to the point [0, 1], checking if it is less than 1 (inside a circle centered at (0,1) with radius 1).
  - **Example**: For x = (0, 0.5), $\|x - [0, 1]\|_2 = \sqrt{(0-0)^2 + (0.5-1)^2} = 0.5$, so the rule is true.

#### Option B
- **A**: $\|x - [0, 1]\|_2 < 1$
  - **Meaning and Example**: Same as C from Option A.
  
- **B**: $\|x - [-1]\|_1 > 2$
  - **Meaning and Example**: Same as B from Option A.

- **C**: $\|x\|_\infty < \frac{1}{4}$
  - **Meaning and Example**: Same as A from Option A.

#### Option C
- **A**: $\|x - [0, 1]\|_2 < 1$
  - **Meaning and Example**: Same as C from Option A.

- **B**: $\|x\|_\infty < \frac{1}{4}$
  - **Meaning and Example**: Same as A from Option A.

- **C**: $\|x - [-1, -1]\|_1 > 2$
  - **Meaning and Example**: Same as B from Option A.

#### Option D
- **A**: $\|x\|_\infty < \frac{1}{4}$
  - **Meaning and Example**: Same as A from Option A.

- **B**: $\|x - [-1]\|_1 < 2$
  - **Meaning**: This measures the Manhattan distance from the point x to the point [-1, -1], checking if it is less than 2.
  - **Example**: For x = (0, 0), $\|x - [-1, -1]\|_1 = |0+1| + |0+1| = 2$, so the rule is false since it's not less than 2.

- **C**: $\|x - [0, 1]\|_2 < 1$
  - **Meaning and Example**: Same as C from Option A.
