
## 1) Which histograms x1, x2, x3, x4 match which boxplots?


## 2) PCA - 

```python
import numpy as np  
  
# Singular values from the matrix S  
singular_values = np.array([149, 118, 53, 42, 3])  
  
# Square the singular values to get the variances  
variances = singular_values**2  
  
# Total variance is the sum of individual variances  
total_variance = np.sum(variances)  
  
# Proportion of variance explained by each principal component  
proportion_variance_explained = variances / total_variance  
  
# Compute cumulative variance explained by the first two and three principal components  
cumulative_variance_two = np.sum(proportion_variance_explained[:2])  
cumulative_variance_three = np.sum(proportion_variance_explained[:3])  
  
# Print the results  
print("Proportion of variance explained by each component:", proportion_variance_explained)  
print("Cumulative variance explained by the first two components:", cumulative_variance_two)  
print("Cumulative variance explained by the first three components:", cumulative_variance_three)  
  
# Check the statements given in the problem  
A = np.sum(proportion_variance_explained[-3:]) < 0.10  
B = proportion_variance_explained[0] > 0.60  
C = np.sum(proportion_variance_explained[-2:]) < 0.04  
D = cumulative_variance_two > 0.85  
  
# Print the truth value of each statement  
print("Statement A is", A)  
print("Statement B is", B)  
print("Statement C is", C)  
print("Statement D is", D)  
  
# Identify the correct statement  
statements = [A, B, C, D]  
statement_labels = ['A', 'B', 'C', 'D']  
for i, statement in enumerate(statements):  
    if statement:  
        print(f"The correct statement is: {statement_labels[i]}")
```


## 3) plot of each observation plotted onto the two first principal directions given in Equation...Which of the following statements best describes the development of  the measurements? 

Info from last 2 problems:
![[Screenshot 2024-04-18 at 17.30.27.png]]
![[Screenshot 2024-04-18 at 17.30.42.png]]


![[Pasted image 20240418173502.png]]

The matrix `S` is a diagonal matrix containing the singular values of the PCA, and the matrix `V` contains the principal directions (eigenvectors). We have to only look at the mesaurements from start to end which goes from 1 to -3 on the y-axis. The operation `-3v2 - 1v2` (it is -1 because we compute difference) is linear combination of the second column of the `V` matrix, which corresponds to the second principal component. 

```python
import numpy as np  
  
# V matrix from the second image  
V = np.array([  
    [-0.3, -0.5,  0.7,  0.2,  0.2],  
    [-0.4,  0.6, -0.0,  0.2,  0.7],  
    [-0.4, -0.4, -0.7,  0.4, -0.0],  
    [-0.6, -0.1, -0.1, -0.8,  0.1],  
    [-0.5,  0.4,  0.2,  0.2, -0.7]  
])  
  
# Extracting the second principal component (v2)  
v2 = V[:, 1]  # This is the second column of V  
  
# Computing -3v2 - 1v2  
result_vector = -3 * v2 - 1 * v2  
  
# Display the result  
print(result_vector)


#result
#[ 2.  -2.4  1.6  0.4 -1.6]
# 2 = x1, -2.4 = x2, etc...
#If negative coefficent negative impact, otherwise positive impact

# We see the temperature goes up, the humidity drops and the light goes up, therefore option A is correct.

```

#EuclianDistances 
## 4) Consider the distances in Table 2. The  class labels C1, C2 (corresponding to {o1, o2, o3, o4, o5} and {o6, o7, o8, o9}) will be predicted using a k-nearest neighbour classifier based on the distances given in Table 2.Suppose we use leave-one-out cross validation  (i.e. the observation that is being predicted is left out) and a 3-nearest neighbour classifier (i.e. k = 3). What is the error rate computed for all N = 9 observations?

![[Pasted image 20240418203729.png]]

## Solution 
The error rate is `2/9` because we have to find the 3 closest values to the values in first column. if majority of the values/neighbours are in the same class it is the correctly classified otherwise its wrong. 
Consider e.g. $O_1$, its closest neighbours are $O_3, O_5, O_6$  where 2/3 are in same class there for it is correctly classified. Whereas, the closest neighbours are $O_7, O_8, O_3$ where 2/3 are in the other class, therefore it is wrongly classified.


## 5) Consider the distances in Table 2 and  suppose we wish to apply mixture modelling and we use the normal density as the mixture distributions:

$$
p(x | \mu, \sigma) = \mathcal{N}(x; \mu, \sigma) = \left( \frac{1}{(2\pi\sigma^2)^{\frac{M}{2}}} \right) e^{-\frac{\|x-\mu\|^2}{2\sigma^2}}.
$$


![[Pasted image 20240418210105.png]]
## Solution
Options A and B are not properly normalized by the number of mixture components. Option C does not use the squared distances. Accordingly option D is the correct answer.

## 6) What is the a.r.d. of observation o9 using K = 2 nearest neighbours?

We wish to compute the average relative KNN density (a.r.d) of observation $O_9$ from the Occupancy dataset described in Table 1 using the distances given in Table 2. Letting $d(x, y)$ denote the Euclidian distance metric the a.r.d. is defined as:

$$
\text{density}(x, K) = \frac{1}{K} \sum_{y \in N(x, K)} d(\bar{x}, y)
$$

$$
\text{a.r.d}(x, K) = \frac{1}{K} \sum_{z \in N(x, K)} \frac{\text{density}(x, K)}{\text{density}(z, K)},
$$

where \( N(x, K) \) is the set of \( K \)-nearest neighbors of \( x \).

## Solution
The nearest neighbour of $O_9$ is $O_6$, $O_8$ and the nearest neighbours of $O_6$ is $O_2$, $O_9$ and for $O_8$ it is $O_7$, $O_9$. The densities are:
$density(O_9, K = 2) = 0.746268656716$
$density(O_6, K = 2) = 0.505050505051$ 
$density(O_8, K = 2) = 0.724637681159$
so: 

$$
\text{a.r.d}(o_g, K = 2) = \frac{0.7463}{\frac{1}{2}(0.5051 + 0.7246)} = 1.2138
$$


```python
import numpy as np  
  
def calculate_density(nearest_distances, K):  
    """  
    Calculate the density for a given observation based on its K-nearest neighbors.  
    Parameters:    - nearest_distances: A list of distances from the observation to its K-nearest neighbors.    - K: The number of nearest neighbors to consider.  
    Returns:    - The density value for the observation.    """    # Ensure the list is sorted and take the K smallest distances  
    if len(nearest_distances) > K:  
        nearest_distances = sorted(nearest_distances)[:K]  
    return 1 / np.mean(nearest_distances)  
  
def calculate_ard(nearest_distances_o, nearest_distances_neighbors, K):  
    """  
    Calculate the average relative density (a.r.d.) for a given observation.  
    Parameters:    - nearest_distances_o: A list of distances from the observation to its K-nearest neighbors.    - nearest_distances_neighbors: A list of lists, each containing distances from each neighbor to its K-nearest neighbors.    - K: The number of nearest neighbors to consider.  
    Returns:    - The average relative density (a.r.d.) for the observation.    """    density_o = calculate_density(nearest_distances_o, K)  
    densities_neighbors = [calculate_density(distances, K) for distances in nearest_distances_neighbors]  
    return density_o / np.mean(densities_neighbors)  
  
# Example usage  

# Remember whatever K is, is the amount of neighbours to find. fo remember to add them.
K = 2  
nearest_distances_o9 = [1.95, 0.73]  # Distances to the two nearest neighbors o6 and o8  
nearest_distances_o6 = [1.95, 2.01]  # Distances from o6 to its two nearest neighbors  
nearest_distances_o8 = [0.73, 2.03]  # Distances from o8 to its two nearest neighbors  
  
# Calculate a.r.d. for o9  
ard_o9 = calculate_ard(nearest_distances_o9, [nearest_distances_o6, nearest_distances_o8], K)  
  
# Print the result  
print(f"Average Relative Density (a.r.d.) of o9: {ard_o9:.4f}")
```

## 7) A hierarchical clustering is applied to the 9 observations in Table 2 using minimum linkage.  Which of the dendrograms shown in Figure 4 corresponds to the clustering? #TODO

![[Pasted image 20240420141416.png]]

### Solution
The correct answer is D, dendrogram 4. $O_8$ and $O_9$ are grouped together in all diagrams. Since the distance from $O_6$ to $O_8$ is lower than the distance from $O_7$ to $O_6$ then $O_6$ should link to $O_8$, $O_9$ before $O_7$. This allows us to rule out dendrogram 1 and dendrogram 2. Finally, $O_3$ and $O_1$ should clearly link together allowing us to rule out dendrogram 3. This leaves only option D.



## 8) What can be concluded about the Cosine similarity of these two observations? (Hint: recall for vectors $x, y$ that $||x - y||^2_2 = ||x||^2_2 + ||y||^2_2 - 2xᵀy$)

In Table 2 is given the pairwise euclidian distances between 9 observations from the Occupancy dataset of Table 1. Suppose the Euclidian norm of observations o2 and o3 is:

$$||o_2|| = \sqrt{\sum_{k=1}^{M} x_{1k}^2} = 3.04, \quad ||o_3|| = \sqrt{\sum_{k=1}^{M} x_{2k}^2} = 1.5$$


To find the cosine similarity between observations $o_2$ and $o_3$, we can use the relationship given in the hint and the norm of the observations:

$$ ||o_2 - o_3||^2_2 = ||o_2||^2_2 + ||o_3||^2_2 - 2 \cdot o_2^To_3 $$

Cosine similarity is defined as:

$$ \text{cosine similarity} = \frac{o_2^To_3}{||o_2|| \cdot ||o_3||} $$

From the provided hint and equation, we can isolate $o_2^To_3$:

$$ 2 \cdot o_2^To_3 = ||o_2||^2_2 + ||o_3||^2_2 - ||o_2 - o_3||^2_2 $$

Given:

$$ ||o_2|| = 3.04 $$
$$ ||o_3|| = 1.5 $$

And from the Table 2 (not shown in the current context, but given earlier), we know the Euclidean distance between $o_2$ and $o_3$:

$$ ||o_2 - o_3|| = 4.40 $$

Now we calculate:

$$ ||o_2||^2_2 = (3.04)^2 = 9.2416 $$
$$ ||o_3||^2_2 = (1.5)^2 = 2.25 $$
$$ ||o_2 - o_3||^2_2 = (4.40)^2 = 19.36 $$

Now we plug these into the equation:

$$ 2 \cdot o_2^To_3 = 9.2416 + 2.25 - 19.36 $$

$$ 2 \cdot o_2^To_3 = 11.4916 - 19.36 $$

$$ 2 \cdot o_2^To_3 = -7.8684 $$

$$ o_2^To_3 = \frac{-7.8684}{2} $$

$$ o_2^To_3 = -3.9342 $$

Then, we calculate the cosine similarity:

$$ \text{cosine similarity} = \frac{-3.9342}{3.04 \cdot 1.5} $$

$$ \text{cosine similarity} = \frac{-3.9342}{4.56} $$

$$ \text{cosine similarity} = -0.8628 $$

Therefore, the correct answer is:

D. cos(o2, o3) ≈ −0.8628

### Script 
```python
import numpy as np  
  
  
def cosine_similarity(norm_o1, norm_o2, euclidean_distance):  
    """  
    Calculate cosine similarity between two observations.  
    Parameters:    - norm_o1: Euclidean norm of the first observation    - norm_o2: Euclidean norm of the second observation    - euclidean_distance: Euclidean distance between the two observations  
    Returns:    - Cosine similarity value    """    # Calculate the squared norms  
    squared_norm_o1 = norm_o1 ** 2  
    squared_norm_o2 = norm_o2 ** 2  
  
    # Calculate the squared Euclidean distance  
    squared_euclidean_distance = euclidean_distance ** 2  
  
    # Calculate the dot product from the norms and Euclidean distance  
    dot_product = (squared_norm_o1 + squared_norm_o2 - squared_euclidean_distance) / 2  
  
    # Calculate the cosine similarity  
    cosine_sim = dot_product / (norm_o1 * norm_o2)  
  
    return cosine_sim  
  
  
# Example usage from task:  
norm_o2 = 3.04  
norm_o3 = 1.5  
euclidean_distance_o2_o3 = 4.40 ## From the table   
# Calculate cosine similarity  
cos_sim = cosine_similarity(norm_o2, norm_o3, euclidean_distance_o2_o3)  
  
print(f"The cosine similarity is approximately {cos_sim:.4f}")
```


## 9) What will Hunt’s algorithm do if classification error is used as impurity measure?
![[Pasted image 20240420141911.png]]

To determine which split Hunt's algorithm would select, we need to calculate the classification error for each potential split and then choose the split that provides the greatest purity gain \( \Delta \). The classification error for a split is calculated by finding the proportion of the majority class within each partition of the split and then subtracting from 1.

The classification error for each node is:

$$
\text{Classification Error} = 1 - \max\left(\frac{\text{Count of Majority Class}}{\text{Total Count}}\right)
$$

To calculate the purity gain \( \Delta \) for a split, we need to calculate the weighted sum of the classification errors for the child nodes and subtract it from the classification error before the split (the parent node).

The dataset before any splits has 129 observations unoccupied (\( y = 0 \)) and 169 observations occupied (\( y = 1 \)). The classification error of the parent node is:

$$
\text{Parent Error} = 1 - \max\left(\frac{129}{129 + 169}, \frac{169}{129 + 169}\right)
$$

Next, we'll calculate the classification error for each split. Here's how to calculate it for Split 1:

For Split 1:

- Node 1 (Temperature < 20): Classification error is $1 - \frac{45}{45 + 1}$
- Node 2 (20 ≤ Temperature ≤ 22): Classification error is $1 - \frac{66}{47 + 66}$
- Node 3 (Temperature > 22): Classification error is $1 - \frac{33}{8 + 33}$

We then weigh these classification errors by the number of observations in each node to get the weighted classification error for Split 1. Finally, we do the same calculations for Splits 2 and 3.

### script

```python
def calculate_classification_error(class_counts):  
    # Calculate the classification error for a node  
    total = sum(class_counts)  
    return 1 - max(class_counts) / total if total > 0 else 0  
  
def calculate_purity_gain(I0, split, n):  
    # Calculate purity gain for a given split  
    weighted_error = sum((count / n) * calculate_classification_error((observed, count - observed))  
                         for observed, count in split)  
    return I0 - weighted_error  
  
# Initial impurity (classification error) for the root node  
I0 = 1 - 1/2  # For a perfectly balanced binary classification  
  
# Number of observations  
n = 200  
  
# Define the splits (occupied, total) from your teacher's solution  
splits_info = {  
    'Split 1': [(45, 46), (66, 113), (33, 41)],  # Split 1: (occupied, total)  
    'Split 2': [(20, 76), (47, 63), (33, 41)],   # Split 2  
    'Split 3': [(0, 25), (23, 78), (77, 97)]     # Split 3  
}  
  
# Calculate purity gains for each split and find the best split  
purity_gains = {split_label: calculate_purity_gain(I0, split, n) for split_label, split in splits_info.items()}  
best_split = max(purity_gains, key=purity_gains.get)  
best_purity_gain = purity_gains[best_split]  
  
# Output the results and the chosen split  
for split_label, purity_gain in purity_gains.items():  
    print(f"Purity gain for {split_label}: {purity_gain:.3f}")  
    if split_label == best_split:  
        print(f"{split_label} is chosen by Hunt's algorithm with a purity gain of {purity_gain:.3f}\n")  
  
# Print comparison between splits  
for split_label, purity_gain in purity_gains.items():  
    if split_label != best_split:  
        print(f"{best_split} is selected over {split_label} because {best_purity_gain:.4f} > {purity_gain:.4f}")
```

## 10

```python
def calculate_naive_bayes(table, f1, f2, f3):
    # Count the number of instances for each class y=0 and y=1
    count_y0 = sum(1 for row in table if row[-1] == 0)
    count_y1 = sum(1 for row in table if row[-1] == 1)

    # Calculate prior probabilities
    prior_y0 = count_y0 / len(table)
    prior_y1 = count_y1 / len(table)

    # Calculate likelihoods for each feature given the class
    likelihood_f1_y0 = sum(1 for row in table if row[0] == f1 and row[-1] == 0) / count_y0
    likelihood_f2_y0 = sum(1 for row in table if row[1] == f2 and row[-1] == 0) / count_y0
    likelihood_f3_y0 = sum(1 for row in table if row[2] == f3 and row[-1] == 0) / count_y0

    likelihood_f1_y1 = sum(1 for row in table if row[0] == f1 and row[-1] == 1) / count_y1
    likelihood_f2_y1 = sum(1 for row in table if row[1] == f2 and row[-1] == 1) / count_y1
    likelihood_f3_y1 = sum(1 for row in table if row[2] == f3 and row[-1] == 1) / count_y1

    # Calculate the unnormalized posteriors for y=0 and y=1
    unnormalized_posterior_y0 = likelihood_f1_y0 * likelihood_f2_y0 * likelihood_f3_y0 * prior_y0
    unnormalized_posterior_y1 = likelihood_f1_y1 * likelihood_f2_y1 * likelihood_f3_y1 * prior_y1

    # Normalize the posteriors so that they sum to 1
    sum_posteriors = unnormalized_posterior_y0 + unnormalized_posterior_y1
    if sum_posteriors == 0:
        return None  # To avoid division by zero if no evidence supports either class
    posterior_y1 = unnormalized_posterior_y1 / sum_posteriors

    return posterior_y1

# The data from Table 4 converted into a list of lists
# The last element in each sub-list is the class label y, derived from the color coding in the image
table_4_data = [
    [0, 1, 1, 0, 1, 0],  # o1
    [0, 0, 1, 0, 0, 0],  # o2
    [1, 0, 0, 0, 1, 0],  # o3
    [1, 0, 0, 1, 1, 0],  # o4
    [1, 0, 0, 1, 0, 0],  # o5
    [1, 1, 0, 1, 1, 0],  # o6
    [1, 0, 1, 1, 0, 1],  # o7
    [1, 0, 1, 1, 1, 1],  # o8
    [0, 1, 1, 1, 1, 1],  # o9
    [1, 0, 1, 0, 0, 1],  # o10
    [0, 1, 1, 0, 0, 1]   # o11
]

# Example observation from the question
f1_observation = 0
f2_observation = 1
f3_observation = 1

# Calculate the probability
probability = calculate_naive_bayes(table_4_data, f1_observation, f2_observation, f3_observation)
probability
```

