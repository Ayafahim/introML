
## 1) Which histograms x1, x2, x3, x4 match which boxplots?


## 2) PCA - 

```python
import numpy as np  
  
# Singular values from the matrix S  
singular_values = np.array([149, 118, 53, 42, 3])  
  
# Square the singular values to get the variances  
variances = singular_values**2  
  
# Total variance is the sum of individual variances  
total_variance = np.sum(variances)  
  
# Proportion of variance explained by each principal component  
proportion_variance_explained = variances / total_variance  
  
# Compute cumulative variance explained by the first two and three principal components  
cumulative_variance_two = np.sum(proportion_variance_explained[:2])  
cumulative_variance_three = np.sum(proportion_variance_explained[:3])  
  
# Print the results  
print("Proportion of variance explained by each component:", proportion_variance_explained)  
print("Cumulative variance explained by the first two components:", cumulative_variance_two)  
print("Cumulative variance explained by the first three components:", cumulative_variance_three)  
  
# Check the statements given in the problem  
A = np.sum(proportion_variance_explained[-3:]) < 0.10  
B = proportion_variance_explained[0] > 0.60  
C = np.sum(proportion_variance_explained[-2:]) < 0.04  
D = cumulative_variance_two > 0.85  
  
# Print the truth value of each statement  
print("Statement A is", A)  
print("Statement B is", B)  
print("Statement C is", C)  
print("Statement D is", D)  
  
# Identify the correct statement  
statements = [A, B, C, D]  
statement_labels = ['A', 'B', 'C', 'D']  
for i, statement in enumerate(statements):  
    if statement:  
        print(f"The correct statement is: {statement_labels[i]}")
```


## S16, Q3) Plot of each observation plotted onto the two first principal directions given in Equation...Which of the following statements best describes the development of  the measurements? 

Info from last 2 problems:
![[Screenshot 2024-04-18 at 17.30.27.png]]
![[Screenshot 2024-04-18 at 17.30.42.png]]


![[Pasted image 20240418173502.png]]

The matrix `S` is a diagonal matrix containing the singular values of the PCA, and the matrix `V` contains the principal directions (eigenvectors). We have to only look at the mesaurements from start to end which goes from 1 to -3 on the y-axis. The operation `-3v2 - 1v2` (it is -1 because we compute difference) is linear combination of the second column of the `V` matrix, which corresponds to the second principal component. 

```python
import numpy as np  
  
# V matrix from the second image  
V = np.array([  
    [-0.3, -0.5,  0.7,  0.2,  0.2],  
    [-0.4,  0.6, -0.0,  0.2,  0.7],  
    [-0.4, -0.4, -0.7,  0.4, -0.0],  
    [-0.6, -0.1, -0.1, -0.8,  0.1],  
    [-0.5,  0.4,  0.2,  0.2, -0.7]  
])  
  
# Extracting the second principal component (v2)  
v2 = V[:, 1]  # This is the second column of V  
  
# Computing -3v2 - 1v2  
result_vector = -3 * v2 - 1 * v2  
  
# Display the result  
print(result_vector)


#result
#[ 2.  -2.4  1.6  0.4 -1.6]
# 2 = x1, -2.4 = x2, etc...
#If negative coefficent negative impact, otherwise positive impact

# We see the temperature goes up, the humidity drops and the light goes up, therefore option A is correct.

```

#EuclianDistances 
## 4) Consider the distances in Table 2. The  class labels C1, C2 (corresponding to {o1, o2, o3, o4, o5} and {o6, o7, o8, o9}) will be predicted using a k-nearest neighbour classifier based on the distances given in Table 2.Suppose we use leave-one-out cross validation  (i.e. the observation that is being predicted is left out) and a 3-nearest neighbour classifier (i.e. k = 3). What is the error rate computed for all N = 9 observations?

![[Pasted image 20240418203729.png]]

## Solution 
The error rate is `2/9` because we have to find the 3 closest values to the values in first column. if majority of the values/neighbours are in the same class it is the correctly classified otherwise its wrong. 
Consider e.g. $O_1$, its closest neighbours are $O_3, O_5, O_6$  where 2/3 are in same class there for it is correctly classified. Whereas, the closest neighbours are $O_7, O_8, O_3$ where 2/3 are in the other class, therefore it is wrongly classified.


## 5) Consider the distances in Table 2 and  suppose we wish to apply mixture modelling and we use the normal density as the mixture distributions:

$$
p(x | \mu, \sigma) = \mathcal{N}(x; \mu, \sigma) = \left( \frac{1}{(2\pi\sigma^2)^{\frac{M}{2}}} \right) e^{-\frac{\|x-\mu\|^2}{2\sigma^2}}.
$$


![[Pasted image 20240418210105.png]]
## Solution
Options A and B are not properly normalized by the number of mixture components. Option C does not use the squared distances. Accordingly option D is the correct answer.

## 6) What is the a.r.d. of observation o9 using K = 2 nearest neighbours?

We wish to compute the average relative KNN density (a.r.d) of observation $O_9$ from the Occupancy dataset described in Table 1 using the distances given in Table 2. Letting $d(x, y)$ denote the Euclidian distance metric the a.r.d. is defined as:

$$
\text{density}(x, K) = \frac{1}{K} \sum_{y \in N(x, K)} d(\bar{x}, y)
$$

$$
\text{a.r.d}(x, K) = \frac{1}{K} \sum_{z \in N(x, K)} \frac{\text{density}(x, K)}{\text{density}(z, K)},
$$

where \( N(x, K) \) is the set of \( K \)-nearest neighbors of \( x \).

## Solution
The nearest neighbour of $O_9$ is $O_6$, $O_8$ and the nearest neighbours of $O_6$ is $O_2$, $O_9$ and for $O_8$ it is $O_7$, $O_9$. The densities are:
$density(O_9, K = 2) = 0.746268656716$
$density(O_6, K = 2) = 0.505050505051$ 
$density(O_8, K = 2) = 0.724637681159$
so: 

$$
\text{a.r.d}(o_g, K = 2) = \frac{0.7463}{\frac{1}{2}(0.5051 + 0.7246)} = 1.2138
$$


```python
import numpy as np  
  
def calculate_density(nearest_distances, K):  
    """  
    Calculate the density for a given observation based on its K-nearest neighbors.  
    Parameters:    - nearest_distances: A list of distances from the observation to its K-nearest neighbors.    - K: The number of nearest neighbors to consider.  
    Returns:    - The density value for the observation.    """    # Ensure the list is sorted and take the K smallest distances  
    if len(nearest_distances) > K:  
        nearest_distances = sorted(nearest_distances)[:K]  
    return 1 / np.mean(nearest_distances)  
  
def calculate_ard(nearest_distances_o, nearest_distances_neighbors, K):  
    """  
    Calculate the average relative density (a.r.d.) for a given observation.  
    Parameters:    - nearest_distances_o: A list of distances from the observation to its K-nearest neighbors.    - nearest_distances_neighbors: A list of lists, each containing distances from each neighbor to its K-nearest neighbors.    - K: The number of nearest neighbors to consider.  
    Returns:    - The average relative density (a.r.d.) for the observation.    """    density_o = calculate_density(nearest_distances_o, K)  
    densities_neighbors = [calculate_density(distances, K) for distances in nearest_distances_neighbors]  
    return density_o / np.mean(densities_neighbors)  
  
# Example usage  

# Remember whatever K is, is the amount of neighbours to find. fo remember to add them.
K = 2  
nearest_distances_o9 = [1.95, 0.73]  # Distances to the two nearest neighbors o6 and o8  
nearest_distances_o6 = [1.95, 2.01]  # Distances from o6 to its two nearest neighbors  
nearest_distances_o8 = [0.73, 2.03]  # Distances from o8 to its two nearest neighbors  
  
# Calculate a.r.d. for o9  
ard_o9 = calculate_ard(nearest_distances_o9, [nearest_distances_o6, nearest_distances_o8], K)  
  
# Print the result  
print(f"Average Relative Density (a.r.d.) of o9: {ard_o9:.4f}")
```

## 7) A hierarchical clustering is applied to the 9 observations in Table 2 using minimum linkage.  Which of the dendrograms shown in Figure 4 corresponds to the clustering? #TODO

![[Pasted image 20240420141416.png]]

### Solution
The correct answer is D, dendrogram 4. $O_8$ and $O_9$ are grouped together in all diagrams. Since the distance from $O_6$ to $O_8$ is lower than the distance from $O_7$ to $O_6$ then $O_6$ should link to $O_8$, $O_9$ before $O_7$. This allows us to rule out dendrogram 1 and dendrogram 2. Finally, $O_3$ and $O_1$ should clearly link together allowing us to rule out dendrogram 3. This leaves only option D.



## 8) What can be concluded about the Cosine similarity of these two observations? (Hint: recall for vectors $x, y$ that $||x - y||^2_2 = ||x||^2_2 + ||y||^2_2 - 2xᵀy$)

In Table 2 is given the pairwise euclidian distances between 9 observations from the Occupancy dataset of Table 1. Suppose the Euclidian norm of observations o2 and o3 is:

$$||o_2|| = \sqrt{\sum_{k=1}^{M} x_{1k}^2} = 3.04, \quad ||o_3|| = \sqrt{\sum_{k=1}^{M} x_{2k}^2} = 1.5$$


To find the cosine similarity between observations $o_2$ and $o_3$, we can use the relationship given in the hint and the norm of the observations:

$$ ||o_2 - o_3||^2_2 = ||o_2||^2_2 + ||o_3||^2_2 - 2 \cdot o_2^To_3 $$

Cosine similarity is defined as:

$$ \text{cosine similarity} = \frac{o_2^To_3}{||o_2|| \cdot ||o_3||} $$

From the provided hint and equation, we can isolate $o_2^To_3$:

$$ 2 \cdot o_2^To_3 = ||o_2||^2_2 + ||o_3||^2_2 - ||o_2 - o_3||^2_2 $$

Given:

$$ ||o_2|| = 3.04 $$
$$ ||o_3|| = 1.5 $$

And from the Table 2 (not shown in the current context, but given earlier), we know the Euclidean distance between $o_2$ and $o_3$:

$$ ||o_2 - o_3|| = 4.40 $$

Now we calculate:

$$ ||o_2||^2_2 = (3.04)^2 = 9.2416 $$
$$ ||o_3||^2_2 = (1.5)^2 = 2.25 $$
$$ ||o_2 - o_3||^2_2 = (4.40)^2 = 19.36 $$

Now we plug these into the equation:

$$ 2 \cdot o_2^To_3 = 9.2416 + 2.25 - 19.36 $$

$$ 2 \cdot o_2^To_3 = 11.4916 - 19.36 $$

$$ 2 \cdot o_2^To_3 = -7.8684 $$

$$ o_2^To_3 = \frac{-7.8684}{2} $$

$$ o_2^To_3 = -3.9342 $$

Then, we calculate the cosine similarity:

$$ \text{cosine similarity} = \frac{-3.9342}{3.04 \cdot 1.5} $$

$$ \text{cosine similarity} = \frac{-3.9342}{4.56} $$

$$ \text{cosine similarity} = -0.8628 $$

Therefore, the correct answer is:

D. cos(o2, o3) ≈ −0.8628

### Script 
```python
import numpy as np  
  
  
def cosine_similarity(norm_o1, norm_o2, euclidean_distance):  
    """  
    Calculate cosine similarity between two observations.  
    Parameters:    - norm_o1: Euclidean norm of the first observation    - norm_o2: Euclidean norm of the second observation    - euclidean_distance: Euclidean distance between the two observations  
    Returns:    - Cosine similarity value    """    # Calculate the squared norms  
    squared_norm_o1 = norm_o1 ** 2  
    squared_norm_o2 = norm_o2 ** 2  
  
    # Calculate the squared Euclidean distance  
    squared_euclidean_distance = euclidean_distance ** 2  
  
    # Calculate the dot product from the norms and Euclidean distance  
    dot_product = (squared_norm_o1 + squared_norm_o2 - squared_euclidean_distance) / 2  
  
    # Calculate the cosine similarity  
    cosine_sim = dot_product / (norm_o1 * norm_o2)  
  
    return cosine_sim  
  
  
# Example usage from task:  
norm_o2 = 3.04  
norm_o3 = 1.5  
euclidean_distance_o2_o3 = 4.40 ## From the table   
# Calculate cosine similarity  
cos_sim = cosine_similarity(norm_o2, norm_o3, euclidean_distance_o2_o3)  
  
print(f"The cosine similarity is approximately {cos_sim:.4f}")
```


## 9) What will Hunt’s algorithm do if classification error is used as impurity measure? #HuntsAlgo


![[Pasted image 20240420141911.png]]

To determine which split Hunt's algorithm would select, we need to calculate the classification error for each potential split and then choose the split that provides the greatest purity gain \( \Delta \). The classification error for a split is calculated by finding the proportion of the majority class within each partition of the split and then subtracting from 1.

The classification error for each node is:

$$
\text{Classification Error} = 1 - \max\left(\frac{\text{Count of Majority Class}}{\text{Total Count}}\right)
$$

To calculate the purity gain \( \Delta \) for a split, we need to calculate the weighted sum of the classification errors for the child nodes and subtract it from the classification error before the split (the parent node).

The dataset before any splits has 129 observations unoccupied (\( y = 0 \)) and 169 observations occupied (\( y = 1 \)). The classification error of the parent node is:

$$
\text{Parent Error} = 1 - \max\left(\frac{129}{129 + 169}, \frac{169}{129 + 169}\right)
$$

Next, we'll calculate the classification error for each split. Here's how to calculate it for Split 1:

For Split 1:

- Node 1 (Temperature < 20): Classification error is $1 - \frac{45}{45 + 1}$
- Node 2 (20 ≤ Temperature ≤ 22): Classification error is $1 - \frac{66}{47 + 66}$
- Node 3 (Temperature > 22): Classification error is $1 - \frac{33}{8 + 33}$

We then weigh these classification errors by the number of observations in each node to get the weighted classification error for Split 1. Finally, we do the same calculations for Splits 2 and 3.

### script

```python
def calculate_classification_error(class_counts):  
    # Calculate the classification error for a node  
    total = sum(class_counts)  
    return 1 - max(class_counts) / total if total > 0 else 0  
  
def calculate_purity_gain(I0, split, n):  
    # Calculate purity gain for a given split  
    weighted_error = sum((count / n) * calculate_classification_error((observed, count - observed))  
                         for observed, count in split)  
    return I0 - weighted_error  
  
# Initial impurity (classification error) for the root node  
I0 = 1 - 1/2  # For a perfectly balanced binary classification  
  
# Number of observations  
n = 200  
  
# Define the splits (occupied, total) from your teacher's solution  
splits_info = {  
    'Split 1': [(45, 46), (66, 113), (33, 41)],  # Split 1: (occupied, total)  
    'Split 2': [(20, 76), (47, 63), (33, 41)],   # Split 2  
    'Split 3': [(0, 25), (23, 78), (77, 97)]     # Split 3  
}  
  
# Calculate purity gains for each split and find the best split  
purity_gains = {split_label: calculate_purity_gain(I0, split, n) for split_label, split in splits_info.items()}  
best_split = max(purity_gains, key=purity_gains.get)  
best_purity_gain = purity_gains[best_split]  
  
# Output the results and the chosen split  
for split_label, purity_gain in purity_gains.items():  
    print(f"Purity gain for {split_label}: {purity_gain:.3f}")  
    if split_label == best_split:  
        print(f"{split_label} is chosen by Hunt's algorithm with a purity gain of {purity_gain:.3f}\n")  
  
# Print comparison between splits  
for split_label, purity_gain in purity_gains.items():  
    if split_label != best_split:  
        print(f"{best_split} is selected over {split_label} because {best_purity_gain:.4f} > {purity_gain:.4f}")
```

## 10) Consider the N = 11 observations from Table 2 and assume the data has been processed to the 11 × 5 binary matrix shown in Table 4. Suppose we consider the first three features f1, f2, f3 and train a Na ̈ıve-Bayes classifier to distinguish between unoccupied and occupied rooms y = 0 and y = 1 based on only these three features. If an observation has f1 = 0, f2 = 1, f3 = 1, what is the probability that the room is occupied, y = 1, according to the Naive-Bayes classifier?

```python
def calculate_naive_bayes(table, f1, f2, f3):
    # Count the number of instances for each class y=0 and y=1
    count_y0 = sum(1 for row in table if row[-1] == 0)
    count_y1 = sum(1 for row in table if row[-1] == 1)

    # Calculate prior probabilities
    prior_y0 = count_y0 / len(table)
    prior_y1 = count_y1 / len(table)

    # Calculate likelihoods for each feature given the class
    likelihood_f1_y0 = sum(1 for row in table if row[0] == f1 and row[-1] == 0) / count_y0
    likelihood_f2_y0 = sum(1 for row in table if row[1] == f2 and row[-1] == 0) / count_y0
    likelihood_f3_y0 = sum(1 for row in table if row[2] == f3 and row[-1] == 0) / count_y0

    likelihood_f1_y1 = sum(1 for row in table if row[0] == f1 and row[-1] == 1) / count_y1
    likelihood_f2_y1 = sum(1 for row in table if row[1] == f2 and row[-1] == 1) / count_y1
    likelihood_f3_y1 = sum(1 for row in table if row[2] == f3 and row[-1] == 1) / count_y1

    # Calculate the unnormalized posteriors for y=0 and y=1
    unnormalized_posterior_y0 = likelihood_f1_y0 * likelihood_f2_y0 * likelihood_f3_y0 * prior_y0
    unnormalized_posterior_y1 = likelihood_f1_y1 * likelihood_f2_y1 * likelihood_f3_y1 * prior_y1

    # Normalize the posteriors so that they sum to 1
    sum_posteriors = unnormalized_posterior_y0 + unnormalized_posterior_y1
    if sum_posteriors == 0:
        return None  # To avoid division by zero if no evidence supports either class
    posterior_y1 = unnormalized_posterior_y1 / sum_posteriors

    return posterior_y1

# The data from Table 4 converted into a list of lists
# The last element in each sub-list is the class label y, derived from the color coding in the image
table_4_data = [
    [0, 1, 1, 0, 1, 0],  # o1
    [0, 0, 1, 0, 0, 0],  # o2
    [1, 0, 0, 0, 1, 0],  # o3
    [1, 0, 0, 1, 1, 0],  # o4
    [1, 0, 0, 1, 0, 0],  # o5
    [1, 1, 0, 1, 1, 0],  # o6
    [1, 0, 1, 1, 0, 1],  # o7
    [1, 0, 1, 1, 1, 1],  # o8
    [0, 1, 1, 1, 1, 1],  # o9
    [1, 0, 1, 0, 0, 1],  # o10
    [0, 1, 1, 0, 0, 1]   # o11
]

# Example observation from the question
f1_observation = 0
f2_observation = 1
f3_observation = 1

# Calculate the probability
probability = calculate_naive_bayes(table_4_data, f1_observation, f2_observation, f3_observation)
probability
```

## 11) Suppose we consider the binary matrix in Table 4 as a market-basket problem consisting of N = 11 ”transactions” o1, . . . , o11 and M = 5 ”items” f1, . . . , f5. Which of the following options represents all itemsets with support greater than 0.32?

```python
def find_itemsets_with_support(table, num_items, min_support_ratio):
    """
    Find all itemsets (single items and pairs) that occur with at least a given support ratio.

    :param table: List of transactions, where each transaction is represented by a list of item flags (0 or 1)
    :param num_items: The number of items M
    :param min_support_ratio: The minimum support ratio
    :return: A list of itemsets that meet the minimum support ratio
    """
    itemsets = []
    num_transactions = len(table)
    min_support_count = min_support_ratio * num_transactions
    
    # Check support for individual items
    for item in range(num_items):
        support_count = sum(transaction[item] for transaction in table)
        if support_count >= min_support_count:
            itemsets.append({f'f{item+1}'})
    
    # Check support for pairs of items
    for item1 in range(num_items):
        for item2 in range(item1+1, num_items):
            support_count = sum(transaction[item1] and transaction[item2] for transaction in table)
            if support_count >= min_support_count:
                itemsets.append({f'f{item1+1}', f'f{item2+1}'})
    
    return itemsets

# M = 5 items, with support greater than 0.32
M = 5
support_ratio = 0.32

# Calculate itemsets with support greater than 0.32, including item pairs
itemsets_with_support = find_itemsets_with_support(table_4_data, M, support_ratio)
itemsets_with_support
```

## 12) We again consider the binary matrix of Table 4 as a market-basket problem consisting of  N = 11 ”transactions” o1, . . . , o11 and M = 5 ”items” f1, . . . , f5. Which of the following rules has the highest confidence?

```python
def calculate_rule_confidence(table, rule_antecedent, rule_consequent):
    """
    Calculate the confidence of a rule in the format antecedent -> consequent.

    :param table: List of transactions, where each transaction is represented by a list of item flags (0 or 1)
    :param rule_antecedent: Tuple representing the antecedent of the rule (e.g., ('f1', 'f5'))
    :param rule_consequent: Tuple representing the consequent of the rule (e.g., ('f4',))
    :return: Confidence of the rule
    """
    num_transactions = len(table)
    antecedent_count = 0
    both_count = 0
    
    # Convert feature names to indices
    antecedent_indices = [int(item[1])-1 for item in rule_antecedent]
    consequent_indices = [int(item[1])-1 for item in rule_consequent]
    
    # Count the occurrences of the antecedent and both antecedent and consequent
    for transaction in table:
        if all(transaction[index] for index in antecedent_indices):
            antecedent_count += 1
            if all(transaction[index] for index in consequent_indices):
                both_count += 1
    
    # Calculate confidence
    confidence = both_count / antecedent_count if antecedent_count else 0
    return confidence

# Rules from the question
rules = [
    (('f3', 'f4'), ('f5',)),
    (('f1', 'f5'), ('f4',)),
    (('f1', 'f4'), ('f5',)),
    (('f2', 'f4'), ('f1',)),
]

# Calculate confidence for each rule
rule_confidences = {rule: calculate_rule_confidence(table_4_data, *rule) for rule in rules}
rule_confidences
```

## 13) We consider the N = 11 observations from Table 4 as 5-dimensional binary vectors. Which one of the following statements is true regarding the Jaccard/cosine similarity and the simple matching coefficient?

```python
import numpy as np

def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

def simple_matching_coefficient(vec1, vec2):
    return sum(1 for i, j in zip(vec1, vec2) if i == j) / len(vec1)

def jaccard_similarity(vec1, vec2):
    intersection = sum(1 for i, j in zip(vec1, vec2) if i == j and i == 1)
    union = sum(1 for i, j in zip(vec1, vec2) if i == 1 or j == 1)
    return intersection / union

# Observations from Table 4 as 5-dimensional binary vectors
o1 = np.array([0, 1, 1, 0, 1])
o2 = np.array([0, 0, 1, 0, 0])
o3 = np.array([1, 0, 0, 0, 1])

# Calculate similarities and coefficients
cos_o1_o2 = cosine_similarity(o1, o2)
smc_o1_o2 = simple_matching_coefficient(o1, o2)
cos_o1_o3 = cosine_similarity(o1, o3)
j_o1_o3 = jaccard_similarity(o1, o3)
cos_o1_o2, smc_o1_o2, cos_o1_o3, j_o1_o3
```


## 14) Consider the Occupancy dataset of Table 1 and suppose we only consider the first four features x1, x2, x3, x4. Suppose we wish to examine which subset of these features can be expected to give the optimal generalization error. In Table 5 is shown how different combinations of features give rise to different error rates on a training and a test set for a classifier. Which one of the following statements is true?

```python
def calculate_humidity_probability(conditional_probs):
    """
    Calculate the probability that a room is humid given that it is occupied.

    Args:
    conditional_probs (dict): A dictionary with the given conditional probabilities.

    Returns:
    float: The calculated probability.
    """
    # P(g2 = 1 | y = 1) = P(g1 = 0, g2 = 1 | y = 1) + P(g1 = 1, g2 = 1 | y = 1)
    probability_humid_occupied = (conditional_probs.get('P(g1=0, g2=1 | y=1)') +
                                  conditional_probs.get('P(g1=1, g2=1 | y=1)'))
    return probability_humid_occupied

# Given conditional probabilities
given_probs = {
    'P(g1=0, g2=1 | y=1)': 0.03,
    'P(g1=1, g2=1 | y=1)': 0.50
}

# Calculate the probability
humidity_probability = calculate_humidity_probability(given_probs)
humidity_probability
```


## 17) 

### Overview of Norms

- **Infinity Norm ($\|x\|_\infty$)**: The infinity norm of a vector x, which is the maximum absolute value of its components. This norm measures the "worst-case" or maximum displacement along any coordinate axis.
  
  **Example**: For x = (1, -3), $\|x\|_\infty = \max(|1|, |-3|) = 3$.

- **Manhattan Norm ($\|x\|_1$)**: The Manhattan (or L1) norm of a vector x, which is the sum of the absolute values of its components. This norm measures the total distance traveled along the axes from the origin to the point.
  
  **Example**: For x = (1, -3), $\|x\|_1 = |1| + |-3| = 4$.

- **Euclidean Norm ($\|x\|_2$)**: The Euclidean (or L2) norm of a vector x, which is the square root of the sum of the squares of its components. It represents the straight-line distance from the origin to the point.
  
  **Example**: For x = (1, -3), $\|x\|_2 = \sqrt{1^2 + (-3)^2} = \sqrt{10}$.

### Explanation of Options

Here are detailed explanations and simple examples for each option's rules applied to decision tree nodes A, B, and C:

#### Option A
- **A**: $\|x\|_\infty < \frac{1}{4}$
  - **Meaning**: This rule checks if the maximum absolute value of either coordinate of x is less than 0.25.
  - **Example**: For x = (0.1, -0.2), $\|x\|_\infty = \max(0.1, 0.2) = 0.2 < 0.25$ so the rule is true.

- **B**: $\|x - [-1]\|_1 > 2$
  - **Meaning**: This measures the Manhattan distance from the point x to the point [-1, -1], checking if it is greater than 2.
  - **Example**: For x = (0, 0), $\|x - [-1, -1]\|_1 = |0+1| + |0+1| = 2$, so the rule is false.

- **C**: $\|x - [0, 1]\|_2 < 1$
  - **Meaning**: This measures the Euclidean distance from the point x to the point [0, 1], checking if it is less than 1 (inside a circle centered at (0,1) with radius 1).
  - **Example**: For x = (0, 0.5), $\|x - [0, 1]\|_2 = \sqrt{(0-0)^2 + (0.5-1)^2} = 0.5$, so the rule is true.

#### Option B
- **A**: $\|x - [0, 1]\|_2 < 1$
  - **Meaning and Example**: Same as C from Option A.
  
- **B**: $\|x - [-1]\|_1 > 2$
  - **Meaning and Example**: Same as B from Option A.

- **C**: $\|x\|_\infty < \frac{1}{4}$
  - **Meaning and Example**: Same as A from Option A.

#### Option C
- **A**: $\|x - [0, 1]\|_2 < 1$
  - **Meaning and Example**: Same as C from Option A.

- **B**: $\|x\|_\infty < \frac{1}{4}$
  - **Meaning and Example**: Same as A from Option A.

- **C**: $\|x - [-1, -1]\|_1 > 2$
  - **Meaning and Example**: Same as B from Option A.

#### Option D
- **A**: $\|x\|_\infty < \frac{1}{4}$
  - **Meaning and Example**: Same as A from Option A.

- **B**: $\|x - [-1]\|_1 < 2$
  - **Meaning**: This measures the Manhattan distance from the point x to the point [-1, -1], checking if it is less than 2.
  - **Example**: For x = (0, 0), $\|x - [-1, -1]\|_1 = |0+1| + |0+1| = 2$, so the rule is false since it's not less than 2.

- **C**: $\|x - [0, 1]\|_2 < 1$
  - **Meaning and Example**: Same as C from Option A.

## 18)

- **Plot C** showing a boundary that closely wraps around each data point and all points seem correctly classified, indeed suggests the characteristics of a **1-Nearest Neighbor (1NN)** classifier. This classifier is known for its complex and highly flexible boundary that adapts to every single point in the dataset.
    
- **Plot D** has a linear boundary dividing the data points into two distinct classes. This type of boundary is typical for **Logistic Regression (LREG)**, which tends to create a straight line or a smooth curve to separate classes in the simplest form.
    
- **Plot B** features distinct rectangular or "boxy" regions which strongly suggests a **Decision Tree (TREE)**. Decision trees perform splits along the axes, leading to piecewise constant decision regions, often appearing as box-like structures.
    

With these identifications:

- Plot A, by elimination and also reflecting its decision boundary, is characterized by a somewhat complex but not entirely high-flexibility curve, suggesting it is likely generated by an **Artificial Neural Network (NNET)** with the capability to model non-linear boundaries, but smoother and less irregular compared to 1NN.

The correct labeling of the plots based on your observations and classifier characteristics would be:

- **Plot A**: NNET
- **Plot B**: TREE
- **Plot C**: 1NN
- **Plot D**: LREG

Thus, the correct option matching these descriptions would be **D**. Plot A is NNET, Plot B is TREE, Plot C is 1NN, Plot D is LREG.


## 19)

#### How to Interpret the Numbers:

Let's apply this understanding to the matrices given in your problem and see how they fit the clusters:

1. **Matrix Σ3 = $\begin{bmatrix} 0.2 & 0.0 \\ 0.0 & 3.5 \end{bmatrix}$**:
   - $\Sigma_{11} = 0.2$: Small variance along the x-axis, meaning the cluster will be narrow horizontally.
   - $\Sigma_{22} = 3.5$: Large variance along the y-axis, meaning the cluster will be much more spread vertically.
   - $\Sigma_{12} = \Sigma_{21} = 0$: No covariance between x and y, so the cluster is aligned with the axes, not tilted.

   **Match**: Perfect for a tall, narrow, vertically aligned cluster like the one on the left in Figure 8.

2. **Matrix Σ2 = $\begin{bmatrix} 1.0 & 0.8 \\ 0.8 & 1.0 \end{bmatrix}$**:
   - $\Sigma_{11} = 1.0$ and $\Sigma_{22} = 1.0$: Similar variance on both x and y axes, indicating a roughly balanced spread in both directions.
   - $\Sigma_{12} = \Sigma_{21} = 0.8$: Significant positive covariance, indicating that as x increases, y also tends to increase. This gives the cluster a tilted orientation from the bottom-left to the top-right.

   **Match**: Ideal for a cluster that stretches diagonally across the plot, like the top-right cluster in Figure 8.

#### Visualizing and Understanding Fit:

Each covariance matrix should be matched with the visual spread and orientation of its respective cluster. In GMMs, you look for a combination of spread (how far out the points extend) and directionality (whether the points form a line or a shape tilted in a particular direction). By comparing the diagonal and off-diagonal values, you can predict these characteristics and match them with observed clusters in the data.

Thus, the correct matching of mean vectors and covariance matrices to the visual clusters, as described, leads to the understanding of why option D correctly assigns each Gaussian component in the given problem.


### Simplifying Gaussian Mixture Model Analysis

Let's simplify this further to help you understand how the Gaussian mixture model (GMM) components match the data in Figure 8, and why option D is the correct choice.

#### Understanding Gaussian Mixture Model Components:

1. **Mean (µ)**: This is where the center of each Gaussian component (cluster) is located. In the 2D plot, it’s where most of the data for that component is clustered around.
2. **Covariance Matrix (Σ)**: This tells us about the spread and orientation of each Gaussian component:
   - Diagonal elements show how spread out the data is along the x and y axes.
   - Off-diagonal elements show if there is any tilt to the data cloud. A non-zero off-diagonal suggests a tilt.

#### Observing the Clusters in Figure 8:

- **Left Cluster**: Tall and narrow (cigar-shaped), stretching vertically.
- **Top-Right Cluster**: Diagonal orientation stretching from the bottom-left to the top-right.
- **Bottom-Right Cluster**: Not immediately relevant for option D analysis but generally rounder and less tilted.

#### Matching Clusters to Parameters:

**Cluster Analysis and Corresponding Parameters:**

1. **Left Cluster (Tall and Narrow)**
   - **Mean (µ)**: We need a µ located around the center of this cluster.
   - **Covariance Matrix (Σ)**: We need a Σ that shows a taller vertical spread (higher variance in the y-direction than in the x-direction), with little to no tilt.

2. **Top-Right Cluster (Diagonal Stretch)**
   - **Mean (µ)**: This cluster’s µ should be near its densest part.
   - **Covariance Matrix (Σ)**: We need a Σ with a noticeable diagonal tilt, indicating a spread from SW to NE.

#### Looking at Option D:

- **Option D**: $p(x) = 0.6 \mathcal{N}(x; \mu_2, \Sigma_3) + 0.2 \mathcal{N}(x; \mu_3, \Sigma_1) + 0.2 \mathcal{N}(x; \mu_1, \Sigma_2)$
   - **µ2** with **Σ3**: From your choices, µ2 is located at (0, 2) and Σ3 is ([0.2, 0.0], [0.0, 3.5]), which fits the left cluster perfectly. Σ3 indicates more spread vertically, matching the tall and narrow shape.
   - **µ1** with **Σ2**: µ1 is (4, 3) and Σ2 is ([1.0, 0.8], [0.8, 1.0]), suitable for the top-right diagonal cluster. Σ2 shows a tilt, consistent with the SW-NE orientation.

This means that for each cluster in the plot, the choice of mean and covariance in option D perfectly matches the visual appearance of the data: **µ2** with **Σ3** for the left cluster, and **µ1** with **Σ2** for the top-right cluster. Therefore, option D is correct based on how the parameters fit the visual data.


## 20)

## Calculating Probability Using EM Algorithm for GMM

Given the problem setup, we are to compute the probability that a new observation (represented as a black cross) belongs to the blue Gaussian component, assuming equal weights for each component. Below is a step-by-step breakdown:

1. **Probabilities from Gaussian Distributions**:
   Each component of the GMM is defined by a Gaussian distribution, and the probability contribution from each is given by:
   $$
   \mathcal{N}(x_0; \mu_i, \sigma_i) = \frac{1}{\sqrt{2\pi \sigma_i^2}} e^{-\frac{(x_0 - \mu_i)^2}{2\sigma_i^2}}
   $$
   where $x_0$ is the observed data point, and $\mu_i, \sigma_i$ are the mean and standard deviation for the $i$-th Gaussian component.

2. **Estimation from the Graph**:
   By visual estimation from the provided graph:
   - $p(x_0|\mu_1, \sigma_1) \approx 0.05$ (for the blue Gaussian)
   - $p(x_0|\mu_2, \sigma_2) \approx 0.25$ (for the red Gaussian)
   - $p(x_0|\mu_3, \sigma_3) \approx 0$ (for the yellow Gaussian, far from the cross)

3. **Equal Weights**:
   Assuming equal weights for each Gaussian component, $\pi_i = \frac{1}{3}$.

4. **Calculating the Posterior Probability**:
   The posterior probability that $x_0$ belongs to the blue component using Bayes' theorem:
   $$
   p(z = 1 | x_0) = \frac{\frac{1}{3}p(x_0|\mu_1, \sigma_1)}{\frac{1}{3}p(x_0|\mu_1, \sigma_1) + \frac{1}{3}p(x_0|\mu_2, \sigma_2) + \frac{1}{3}p(x_0|\mu_3, \sigma_3)}
   $$
   Simplifying, it results in:
   $$
   p(z = 1 | x_0) = \frac{0.05}{0.30} \approx 0.17
   $$

This indicates that the probability of the black cross belonging to the blue component is approximately 0.17.

```python
# Define the densities at x0 for each Gaussian component (from the graph)
densities_at_x0 = {
    'blue': 0.05,
    'red': 0.25,
    'yellow': 0
}

# Assuming equal weights for simplicity
weights = {
    'blue': 1/3,
    'red': 1/3,
    'yellow': 1/3
}

# Calculate the weighted densities (numerator components for Bayes' theorem)
weighted_densities = {k: weights[k] * densities_at_x0[k] for k in densities_at_x0}

# Calculate the total probability (denominator of Bayes' theorem)
total_probability = sum(weighted_densities.values())

# Calculate the posterior probabilities for each component
posterior_probabilities = {k: weighted_densities[k] / total_probability for k in weighted_densities}

# Output the results
print("Weighted densities:", weighted_densities)
print("Total probability:", total_probability)
print("Posterior probabilities:", posterior_probabilities)

# Specifically output the posterior probability for the blue component
print("Posterior probability that x0 belongs to the blue component:", posterior_probabilities['blue'])
```

## 21)

## Determining Correct Weight Settings for Softmax Regression

To calculate the scores for each class using the weight vectors for each option, and determine which option results in the highest score for Class 1, we use the example point $x = [0, 1]^T$, where $x_1 = 0$ and $x_2 = 1$.

### How Scores Are Calculated
In softmax regression, the score for each class is calculated by taking the dot product of the weight vector for that class and the input vector $x$. The dot product sums the products of corresponding elements of the two vectors.

### Formula
For a weight vector $w = [w_1, w_2]$ and input vector $x = [x_1, x_2]$, the score is computed as:
$$
y = w_1 \times x_1 + w_2 \times x_2
$$

Given $x_1 = 0$ and $x_2 = 1$, the formula simplifies to:
$$
y = w_2
$$
Thus, for our specific example point, the score for each class depends solely on the second component of the weight vector for that class.

### Calculations for Each Option
Using the simplified formula, here's how the scores are calculated for each class under Option C:

- Weight vectors:
  - $w_1 = [1, 1]$
  - $w_2 = [-1, -1]$
  - $w_3 = [1, -1]$

- Scores for $x = [0, 1]$:
  - $y_1 = 1$ (from $w_1$)
  - $y_2 = -1$ (from $w_2$)
  - $y_3 = -1$ (from $w_3$)

### Conclusion
For $x = [0, 1]$, the score for Class 1 is the highest (1 compared to -1 for the other classes). Consequently, the softmax function would assign the highest probability to Class 1 for this point, consistent with the visual distribution of classes in the provided graph where Class 1 is predominant in the upper area (positive $x_2$).

**Option C** correctly classifies this point into Class 1, aligning with the observed class distributions in the graph, thus confirming that the weights specified in Option C are appropriately set.


```python
import numpy as np

def softmax(scores):
    """Calculate softmax probabilities from scores.
    
    Args:
        scores (np.array): Array of scores for each class.
    
    Returns:
        np.array: Softmax probabilities.
    """
    exp_scores = np.exp(scores)
    return exp_scores / np.sum(exp_scores)

def calculate_class_scores(weights, input_point):
    """Calculate class scores for a given input using weight vectors.
    
    Args:
        weights (np.array): Weight vectors for each class.
        input_point (np.array): The input features as a vector.
    
    Returns:
        np.array: Scores for each class.
    """
    return np.dot(weights, input_point)

def evaluate_options(input_point):
    # Weight configurations for each option
    options = {
        "A": np.array([[-1, -1], [1, 1], [1, -1]]),
        "B": np.array([[1, -1], [-1, -1], [1, 1]]),
        "C": np.array([[1, 1], [-1, -1], [1, -1]]),
        "D": np.array([[-1, -1], [1, 1], [-1, 1]])
    }

    # Evaluate each option
    for option, weights in options.items():
        scores = calculate_class_scores(weights, input_point)
        probabilities = softmax(scores)
        print(f"Option {option}:")
        print("  Scores:", scores)
        print("  Softmax probabilities:", probabilities)
        print()

def main():
    # Define the input point, e.g., x = [0, 1]
    input_point = np.array([0, 1])
    
    # Evaluate all options
    evaluate_options(input_point)

if __name__ == "__main__":
    main()
```

## 23)
## Step-by-Step Verification Process for K-means Clustering

Verifying if a clustering is a converged state of the K-means algorithm involves ensuring that each data point is assigned to the nearest centroid. This confirms the clustering is stable, meaning no point would change clusters if the algorithm were to continue running. Here’s a detailed breakdown:

### 1. **Calculate Centroids**
Each cluster’s centroid is computed as the mean of all points within that cluster. For a cluster containing points $x_1, x_2, \dots, x_n$, the centroid $c$ is calculated by:
$$
c = \frac{x_1 + x_2 + \dots + x_n}{n}
$$

### 2. **Check Assignments**
Verify that each data point belongs to the cluster with the closest centroid. This involves calculating the distance from each data point to every centroid and ensuring the data point is assigned to the cluster whose centroid has the minimum distance.

### 3. **Verifying Stability**
A clustering is considered stable if every data point is nearest to the centroid of the cluster to which it is assigned, rather than any other centroid. If this condition holds, it indicates that the clustering has likely reached a converged state because no points would shift to a different cluster upon further iterations.

### Example to Validate Option C

Consider the following cluster assignments and verify them:

**Clusters:**
- Cluster A: $\{1, 3, 4\}$
- Cluster B: $\{6, 7, 8\}$
- Cluster C: $\{13, 15, 16, 17\}$

**Centroids Calculation:**
- Cluster A: $\frac{1+3+4}{3} = 2.67$
- Cluster B: $\frac{6+7+8}{3} = 7$
- Cluster C: $\frac{13+15+16+17}{4} = 15.25$

**Distance Check (simplified):**
- **Point 1:** Closer to 2.67 than to 7 or 15.25, confirming its cluster.
- **Point 6:** Closer to 7 than to 2.67 or 15.25, confirming its cluster.
- **Point 13:** Closer to 15.25 than to 7 or 2.67, confirming its cluster.

Each point in each cluster is verified to be closest to its own cluster’s centroid, which suggests stability and correctness in terms of K-means convergence.

```python
import numpy as np  
data = np.array([1, 3, 4, 6, 7, 8, 13, 15, 16, 17])  
  
def calculate_centroid(cluster):  
    return np.mean(cluster)  
  
def validate_clusters(data_points, cluster_options):  
    results = {}  
    for option, clusters in cluster_options.items():  
        centroids = [calculate_centroid(cluster) for cluster in clusters]  
        valid = True  
        for cluster in clusters:  
            for point in cluster:  
                distances = [abs(point - centroid) for centroid in centroids]  
                if not cluster == clusters[np.argmin(distances)]:  
                    valid = False  
                    break            if not valid:  
                break  
        results[option] = valid  
    return results  
  
# Provided cluster options  
cluster_options = {  
    "A": [[1, 3], [4, 6, 7], [8, 13, 15, 16], [17]],  
    "B": [[1], [3, 4, 6], [7, 8], [13, 15, 16, 17]],  
    "C": [[1, 3, 4], [6, 7, 8], [13, 15, 16, 17]],  
    "D": [[1, 3, 4], [6, 7], [8, 13], [15], [16, 17]],  
}  
  
# Check if the provided clustering options could be converged states  
results = validate_clusters(data, cluster_options)  
for option, is_valid in results.items():  
    print(f"Option {option}: {'Valid' if is_valid else 'Invalid'}")
```

## 24)

## Analysis of Similarity Measure \( s(x, y) \)

The similarity measure given is:

$$
s(x, y) = \sqrt{\frac{\|x\|^2 \|y\|^2 - (x^T y)^2}{\|x\|^2 \|y\|^2}}
$$

### Scale Invariance
To determine if \( s(x, y) \) is scale invariant, we analyze the effect of scaling \( x \) by any scalar \( \alpha > 0 \):

$$
s(\alpha x, y) = \sqrt{\frac{\|\alpha x\|^2 \|y\|^2 - ((\alpha x)^T y)^2}{\|\alpha x\|^2 \|y\|^2}}
$$

With \( \|\alpha x\| = \alpha \|x\| \) and \( (\alpha x)^T y = \alpha x^T y \), the formula simplifies to:

$$
s(\alpha x, y) = \sqrt{\frac{(\alpha^2 \|x\|^2) \|y\|^2 - (\alpha^2 (x^T y)^2)}{(\alpha^2 \|x\|^2) \|y\|^2}}
$$

$$
s(\alpha x, y) = \sqrt{\frac{\alpha^2 (\|x\|^2 \|y\|^2 - (x^T y)^2)}{\alpha^2 \|x\|^2 \|y\|^2}}
$$

$$
s(\alpha x, y) = \sqrt{\frac{\|x\|^2 \|y\|^2 - (x^T y)^2}{\|x\|^2 \|y\|^2}} = s(x, y)
$$

The \( \alpha^2 \) terms cancel out, indicating that \( s(x, y) \) is scale invariant.

### Translation Invariance
To test for translation invariance, we analyze the effect of translating \( x \) by adding a scalar \( \beta \):

$$
s(x + \beta, y) = \sqrt{\frac{\|x + \beta\|^2 \|y\|^2 - ((x + \beta)^T y)^2}{\|x + \beta\|^2 \|y\|^2}}
$$

The expressions \( \|x + \beta\| \) and \( (x + \beta)^T y \) do not simplify in a way that \( \beta \) cancels out, affecting the result. Thus, \( s(x, y) \) is **not** translation invariant.

### Conclusion
The analysis concludes:

- **Statement A**: True, \( s \) is scale invariant.
- **Statement B**: False, \( s \) is not translation invariant.
- **Statement C**: False, \( s \) is not both translation and scale invariant.
- **Statement D**: False, as \( s \) is scale invariant but not translation invariant.

The correct answer is **A: \( s \) is scale invariant**.


```python
import numpy as np  
  
def similarity_measure(x, y):  
    norm_x = np.linalg.norm(x)  
    norm_y = np.linalg.norm(y)  
    dot_product = np.dot(x, y)  
    return np.sqrt((norm_x**2 * norm_y**2 - dot_product**2) / (norm_x**2 * norm_y**2))  
  
# Example vectors  
x = np.array([1, 2, 3])  
y = np.array([4, 5, 6])  
  
# Scale invariance check  
alpha = 2  
scale_invariant = similarity_measure(alpha * x, y) == similarity_measure(x, y)  
  
# Translation invariance check  
beta = 5  
translation_invariant = similarity_measure(x + beta, y) == similarity_measure(x, y)  
  
print(f"Scale invariant: {scale_invariant}")  
print(f"Translation invariant: {translation_invariant}")
```

## 25)  AdaBoost Weight Update Explanation #AdaBoost

AdaBoost modifies the weights of observations based on classifier performance. Below is a step-by-step breakdown of how weights were updated after the first iteration of AdaBoost, where a logistic regression classifier misclassified two observations.

### Steps in AdaBoost Weight Calculation

1. **Initial Observation Weights:**
   - Initially, all observations have equal weights, $w_j = \frac{1}{6}$.

2. **Error Calculation ($\epsilon_i$):**
   - The error of the classifier is calculated as the sum of the weights of the misclassified observations.
   - With 2 out of 6 observations misclassified, $\epsilon_i = \frac{1}{3}$.

3. **Calculate $\alpha_i$:**
   - The weight update factor, $\alpha_i$, is computed using the formula:
     $$
     \alpha_i = \frac{1}{2} \log\left(\frac{1 - \epsilon_i}{\epsilon_i}\right)
     $$
   - Substituting $\epsilon_i = \frac{1}{3}$ gives:
     $$
     \alpha_1 = \frac{1}{2} \log(2)
     $$

4. **Update Weights:**
   - Weights are updated based on classification accuracy:
     - Correctly classified: $w \propto e^{-\alpha_1}$
     - Incorrectly classified: $w \propto e^{\alpha_1}$
   - Applying these factors results in:
     - Correct: $w \propto \frac{1}{\sqrt{2}}$
     - Incorrect: $w \propto \sqrt{2}$

5. **Normalization of Weights:**
   - Weights are normalized to ensure they sum to 1. The initial proportional weights based on correct and incorrect classifications are simplified and then normalized, leading to:
     $$
     w = \left[0.125, 0.250, 0.125, 0.125, 0.125, 0.250\right]
     $$

### Conclusion

The AdaBoost algorithm effectively adjusts the weights, increasing them for misclassified observations and decreasing for correctly classified, ensuring subsequent classifiers focus more on difficult cases. The correct answer based on the provided update and normalization process is Option B, which accurately reflects the updated weights: $[0.125, 0.250, 0.125, 0.125, 0.125, 0.250]$.


```python
import numpy as np

# Initial weights
N = 6
weights = np.full(N, 1/N)

# Mock results from a classifier (True = correctly classified, False = misclassified)
results = np.array([True, False, True, True, True, False])

# Calculate error rate
epsilon = np.sum(weights[~results])

# Calculate classifier performance
alpha = 0.5 * np.log((1 - epsilon) / epsilon)

# Update weights
weights[results] *= np.exp(-alpha)
weights[~results] *= np.exp(alpha)

# Normalize weights
weights /= np.sum(weights)

print("Updated Weights:", weights)
```


## 26)
