
## 1) Which histograms x1, x2, x3, x4 match which boxplots?


## 2) PCA - 

```python
import numpy as np  
  
# Singular values from the matrix S  
singular_values = np.array([149, 118, 53, 42, 3])  
  
# Square the singular values to get the variances  
variances = singular_values**2  
  
# Total variance is the sum of individual variances  
total_variance = np.sum(variances)  
  
# Proportion of variance explained by each principal component  
proportion_variance_explained = variances / total_variance  
  
# Compute cumulative variance explained by the first two and three principal components  
cumulative_variance_two = np.sum(proportion_variance_explained[:2])  
cumulative_variance_three = np.sum(proportion_variance_explained[:3])  
  
# Print the results  
print("Proportion of variance explained by each component:", proportion_variance_explained)  
print("Cumulative variance explained by the first two components:", cumulative_variance_two)  
print("Cumulative variance explained by the first three components:", cumulative_variance_three)  
  
# Check the statements given in the problem  
A = np.sum(proportion_variance_explained[-3:]) < 0.10  
B = proportion_variance_explained[0] > 0.60  
C = np.sum(proportion_variance_explained[-2:]) < 0.04  
D = cumulative_variance_two > 0.85  
  
# Print the truth value of each statement  
print("Statement A is", A)  
print("Statement B is", B)  
print("Statement C is", C)  
print("Statement D is", D)  
  
# Identify the correct statement  
statements = [A, B, C, D]  
statement_labels = ['A', 'B', 'C', 'D']  
for i, statement in enumerate(statements):  
    if statement:  
        print(f"The correct statement is: {statement_labels[i]}")
```


## 3) plot of each observation plotted onto the two first principal directions given in Equation...Which of the following statements best describes the development of  the measurements? 

Info from last 2 problems:
![[Screenshot 2024-04-18 at 17.30.27.png]]
![[Screenshot 2024-04-18 at 17.30.42.png]]


![[Pasted image 20240418173502.png]]

The matrix `S` is a diagonal matrix containing the singular values of the PCA, and the matrix `V` contains the principal directions (eigenvectors). We have to only look at the mesaurements from start to end which goes from 1 to -3 on the y-axis. The operation `-3v2 - 1v2` (it is -1 because we compute difference) is linear combination of the second column of the `V` matrix, which corresponds to the second principal component. 

```python
import numpy as np  
  
# V matrix from the second image  
V = np.array([  
    [-0.3, -0.5,  0.7,  0.2,  0.2],  
    [-0.4,  0.6, -0.0,  0.2,  0.7],  
    [-0.4, -0.4, -0.7,  0.4, -0.0],  
    [-0.6, -0.1, -0.1, -0.8,  0.1],  
    [-0.5,  0.4,  0.2,  0.2, -0.7]  
])  
  
# Extracting the second principal component (v2)  
v2 = V[:, 1]  # This is the second column of V  
  
# Computing -3v2 - 1v2  
result_vector = -3 * v2 - 1 * v2  
  
# Display the result  
print(result_vector)


#result
#[ 2.  -2.4  1.6  0.4 -1.6]
# 2 = x1, -2.4 = x2, etc...
#If negative coefficent negative impact, otherwise positive impact

# We see the temperature goes up, the humidity drops and the light goes up, therefore option A is correct.

```

#EuclianDistances 
## Consider the distances in Table 2. The  class labels C1, C2 (corresponding to {o1, o2, o3, o4, o5} and {o6, o7, o8, o9}) will be predicted using a k-nearest neighbour classifier based on the distances given in Table 2.Suppose we use leave-one-out cross validation  (i.e. the observation that is being predicted is left out) and a 3-nearest neighbour classifier (i.e. k = 3). What is the error rate computed for all N = 9 observations?

![[Pasted image 20240418203729.png]]

## Solution 
The error rate is `2/9` because we have to find the 3 closest values to the values in first column. if majority of the values/neighbours are in the same class it is the correctly classified otherwise its wrong. 
Consider e.g. $O_1$, its closest neighbours are $O_3, O_5, O_6$  where 2/3 are in same class there for it is correctly classified. Whereas, the closest neighbours are $O_7, O_8, O_3$ where 2/3 are in the other class, therefore it is wrongly classified.


## Consider the distances in Table 2 and  suppose we wish to apply mixture modelling and we use the normal density as the mixture distributions:

$$
p(x | \mu, \sigma) = \mathcal{N}(x; \mu, \sigma) = \left( \frac{1}{(2\pi\sigma^2)^{\frac{M}{2}}} \right) e^{-\frac{\|x-\mu\|^2}{2\sigma^2}}.
$$


![[Pasted image 20240418210105.png]]
## Solution
Options A and B are not properly normalized by the number of mixture components. Option C does not use the squared distances. Accordingly option D is the correct answer.

## What is the a.r.d. of observation o9 using K = 2 nearest neighbours?

We wish to compute the average relative KNN density (a.r.d) of observation $O_9$ from the Occupancy dataset described in Table 1 using the distances given in Table 2. Letting $d(x, y)$ denote the Euclidian distance metric the a.r.d. is defined as:

$$
\text{density}(x, K) = \frac{1}{K} \sum_{y \in N(x, K)} d(\bar{x}, y)
$$

$$
\text{a.r.d}(x, K) = \frac{1}{K} \sum_{z \in N(x, K)} \frac{\text{density}(x, K)}{\text{density}(z, K)},
$$

where \( N(x, K) \) is the set of \( K \)-nearest neighbors of \( x \).

## Solution
The nearest neighbour of $O_9$ is $O_6$, $O_8$ and the nearest neighbours of $O_6$ is $O_2$, $O_9$ and for $O_8$ it is $O_7$, $O_9$. The densities are:
$density(O_9, K = 2) = 0.746268656716$
$density(O_6, K = 2) = 0.505050505051$ 
$density(O_8, K = 2) = 0.724637681159$
so: 

$$
\text{a.r.d}(o_g, K = 2) = \frac{0.7463}{\frac{1}{2}(0.5051 + 0.7246)} = 1.2138
$$


